{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc80870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import asyncio\n",
    "from agents.extensions.visualization import draw_graph\n",
    "import os \n",
    "from openai import AsyncOpenAI \n",
    "from dotenv import load_dotenv  \n",
    "load_dotenv() \n",
    "from agents import (\n",
    "    Agent,\n",
    "    Runner, \n",
    "    set_tracing_disabled,OpenAIChatCompletionsModel,enable_verbose_stdout_logging\n",
    ")   \n",
    " \n",
    "\n",
    "# enable_verbose_stdout_logging()\n",
    "set_tracing_disabled(disabled=True)\n",
    "API_KEY=os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if  not API_KEY  :\n",
    "    raise ValueError(\"Please set EXAMPLE_BASE_URL, EXAMPLE_API_KEY, EXAMPLE_MODEL_NAME via env var or code.\")\n",
    "\n",
    " \n",
    "client = AsyncOpenAI(base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",api_key=API_KEY,)\n",
    "\n",
    "model = OpenAIChatCompletionsModel(model=\"gemini-2.0-flash\",openai_client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399548fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's an explanation of what AI is, in both English and Urdu:\n",
      "\n",
      "**English:**\n",
      "\n",
      "AI stands for **Artificial Intelligence**. It's a broad field of computer science that aims to create machines that can perform tasks that typically require human intelligence. These tasks include:\n",
      "\n",
      "*   **Learning:** Acquiring information and rules for using the information.\n",
      "*   **Reasoning:** Using rules to reach conclusions (either definite or approximate).\n",
      "*   **Problem-solving:** Formulating problems, generating and evaluating possible solutions.\n",
      "*   **Perception:** Using sensory inputs to deduce the aspects of the world.\n",
      "*   **Language understanding:** Understanding and generating human language.\n",
      "\n",
      "AI can be implemented in many different ways, from simple rule-based systems to complex neural networks. You encounter AI in everyday life through things like:\n",
      "\n",
      "*   **Virtual assistants:** (Siri, Alexa, Google Assistant)\n",
      "*   **Recommendation systems:** (Netflix, Amazon)\n",
      "*   **Spam filters:** (Email)\n",
      "*   **Self-driving cars:** (Tesla, Waymo)\n",
      "\n",
      "**Urdu:**\n",
      "\n",
      "AI کا مطلب ہے **مصنوعی ذہانت** (Masnooi Zehanat). یہ کمپیوٹر سائنس کا ایک وسیع میدان ہے جس کا مقصد ایسی مشینیں بنانا ہے جو ایسے کام انجام دے سکیں جن کے لیے عام طور پر انسانی ذہانت کی ضرورت ہوتی ہے۔ ان کاموں میں شامل ہیں:\n",
      "\n",
      "*   **سیکھنا (Seekhna):** معلومات اور معلومات کو استعمال کرنے کے قواعد حاصل کرنا۔\n",
      "*   **استدلال (Istedlal):** نتائج تک پہنچنے کے لیے قواعد کا استعمال کرنا (یقینی یا تخمینی طور پر)۔\n",
      "*   **مسائل کا حل (Masail ka Hal):** مسائل کی تشکیل کرنا، ممکنہ حل پیدا کرنا اور ان کا جائزہ لینا۔\n",
      "*   **ادراک (Idraak):** دنیا کے پہلوؤں کا اندازہ لگانے کے لیے حسی ان پٹ کا استعمال کرنا۔\n",
      "*   **زبانی فہم (Zabani Fehm):** انسانی زبان کو سمجھنا اور تیار کرنا۔\n",
      "\n",
      "AI کو مختلف طریقوں سے نافذ کیا جا سکتا ہے، سادہ قاعدہ پر مبنی نظاموں سے لے کر پیچیدہ نیورل نیٹ ورکس تک۔ آپ روزمرہ کی زندگی میں AI کا سامنا ان چیزوں کے ذریعے کرتے ہیں جیسے:\n",
      "\n",
      "*   **ورچوئل اسسٹنٹ (Virtual Assistant):** (Siri, Alexa, Google Assistant)\n",
      "*   **سفارشاتی نظام (Sifarishati Nizam):** (Netflix, Amazon)\n",
      "*   **اسپام فلٹر (Spam Filter):** (ای میل)\n",
      "*   **خود چلنے والی کاریں (Khud Chalne Wali Carain):** (Tesla, Waymo)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled,ModelSettings\n",
    "\n",
    "set_tracing_disabled(disabled=False)\n",
    "agent = Agent(\n",
    "    name=\"\",\n",
    "    instructions=\"You are a helpful assistant. Your task is to respond in both Urdu and English.\",\n",
    "    model=model,\n",
    "    model_settings=ModelSettings(temperature=0)\n",
    ")\n",
    "\n",
    "resp = await Runner.run(agent, \"what is ai\")\n",
    "print(resp.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602b6c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a breakdown of what AI is, in both English and Urdu:\n",
      "\n",
      "**English:**\n",
      "\n",
      "AI stands for **Artificial Intelligence**.  It's a broad field of computer science that aims to create machines that can perform tasks that typically require human intelligence. These tasks include:\n",
      "\n",
      "*   **Learning:**  Acquiring information and rules for using the information.\n",
      "*   **Reasoning:**  Using logic and inference to draw conclusions.\n",
      "*   **Problem-solving:**  Finding solutions to complex issues.\n",
      "*   **Perception:**  Interpreting sensory input (like images, sounds, or text).\n",
      "*   **Natural Language Processing (NLP):** Understanding and generating human language.\n",
      "\n",
      "AI is not a single thing but rather a collection of technologies and techniques. Some common approaches include machine learning (where systems learn from data), deep learning (a more advanced form of machine learning using neural networks), and rule-based systems (where systems follow pre-defined rules).\n",
      "\n",
      "**Urdu:**\n",
      "\n",
      "AI کا مطلب ہے **مصنوعی ذہانت** (Masnooi Zehanat)۔ یہ کمپیوٹر سائنس کا ایک وسیع شعبہ ہے جس کا مقصد ایسی مشینیں بنانا ہے جو ایسے کام کر سکیں جن کے لیے عام طور پر انسانی ذہانت کی ضرورت ہوتی ہے۔ ان کاموں میں شامل ہیں:\n",
      "\n",
      "*   **سیکھنا (Seekhna):** معلومات اور معلومات کے استعمال کے قواعد حاصل کرنا۔\n",
      "*   **استدلال (Istidlal):** نتائج اخذ کرنے کے لیے منطق اور استدلال کا استعمال کرنا۔\n",
      "*   **مسئلہ حل کرنا (Masla Hal Karna):** پیچیدہ مسائل کا حل تلاش کرنا۔\n",
      "*   **ادراک (Idraak):** حسی ان پٹ (جیسے تصاویر، آوازیں یا متن) کی تشریح کرنا۔\n",
      "*   **قدرتی لسانی پروسیسنگ (NLP) (Qudrati Lisani Processing):** انسانی زبان کو سمجھنا اور تیار کرنا۔\n",
      "\n",
      "AI کوئی ایک چیز نہیں ہے بلکہ ٹیکنالوجیز اور تکنیکوں کا ایک مجموعہ ہے۔ کچھ عام طریقوں میں مشین لرننگ (جہاں نظام ڈیٹا سے سیکھتے ہیں)، ڈیپ لرننگ (نیورل نیٹ ورکس کا استعمال کرتے ہوئے مشین لرننگ کی ایک زیادہ جدید شکل)، اور قاعدہ پر مبنی نظام (جہاں نظام پہلے سے طے شدہ قواعد پر عمل کرتے ہیں) شامل ہیں۔\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled,ModelSettings\n",
    "\n",
    "set_tracing_disabled(disabled=False)\n",
    "agent = Agent(\n",
    "    name=\"\",\n",
    "    instructions=\"You are a helpful assistant. Your task is to respond in both Urdu and English.\",\n",
    "    model=model,\n",
    "    model_settings=ModelSettings(top_p=.9)\n",
    ")\n",
    "\n",
    "resp = await Runner.run(agent, \"what is ai\")\n",
    "print(resp.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cabd46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tools calls\n",
      "AI is a broad field encompassing the development of computer systems that can perform tasks that typically require human intelligence. These tasks include learning, problem-solving, decision-making, and perception.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled,ModelSettings,function_tool\n",
    "\n",
    "set_tracing_disabled(disabled=False)\n",
    "@function_tool\n",
    "def findpriminster(country:str):\n",
    "    print('tools calls')\n",
    "    return f\"Priminister of {country} is Imran Khan\"\n",
    "agent = Agent(\n",
    "    name=\"\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    model=model,\n",
    "    model_settings=ModelSettings(tool_choice='required'),\n",
    "    tools=[findpriminster]\n",
    ")\n",
    "\n",
    "resp = await Runner.run(agent, \"what is ai\")\n",
    "print(resp.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2667b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tools calls\n",
      "tools calls prisident\n",
      "Priminister of pakistan is Imran Khan\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled,ModelSettings,function_tool\n",
    "\n",
    "set_tracing_disabled(disabled=False)\n",
    "@function_tool\n",
    "def findpriminster(country:str):\n",
    "    print('tools calls')\n",
    "    return f\"Priminister of {country} is Imran Khan\"\n",
    "\n",
    "@function_tool\n",
    "def findprisident(country:str):\n",
    "    print('tools calls prisident')\n",
    "    return f\"Prisident of {country} is asif ali zardari\"\n",
    "agent = Agent(\n",
    "    name=\"\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    model=model,\n",
    "    model_settings=ModelSettings(tool_choice='required'),\n",
    "    tools=[findpriminster,findprisident],\n",
    "    tool_use_behavior='stop_on_first_tool'\n",
    ")\n",
    "\n",
    "resp = await Runner.run(agent, \"who are   of pakistan\",max_turns=1)\n",
    "print(resp.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ddd504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data. Data science is related to data mining, machine learning and big data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled,ModelSettings,function_tool\n",
    "\n",
    "set_tracing_disabled(disabled=False)\n",
    "@function_tool\n",
    "def findpriminster(country:str):\n",
    "    print('tools calls')\n",
    "    return f\"Priminister of {country} is Imran Khan\"\n",
    "\n",
    "@function_tool\n",
    "def findprisident(country:str):\n",
    "    print('tools calls prisident')\n",
    "    return f\"Prisident of {country} is asif ali zardari\"\n",
    "agent = Agent(\n",
    "    name=\"\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    model=model,\n",
    "    # model_settings=ModelSettings(tool_choice='required'),\n",
    "    tools=[findpriminster,findprisident],\n",
    "    tool_use_behavior='stop_on_first_tool'\n",
    ")\n",
    "\n",
    "resp = await Runner.run(agent, \"what is datascience\",max_turns=1)\n",
    "print(resp.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47d00318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tools calls prisident\n",
      "Prisident of pakistan is asif ali zardari\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled,ModelSettings,function_tool\n",
    "\n",
    "set_tracing_disabled(disabled=False)\n",
    "@function_tool\n",
    "def findpriminster(country:str):\n",
    "    print('tools calls')\n",
    "    return f\"Priminister of {country} is Imran Khan\"\n",
    "\n",
    "@function_tool\n",
    "def findprisident(country:str):\n",
    "    print('tools calls prisident')\n",
    "    return f\"Prisident of {country} is asif ali zardari\"\n",
    "agent = Agent(\n",
    "    name=\"\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    model=model,\n",
    "    model_settings=ModelSettings(tool_choice='findprisident'),\n",
    "    tools=[findpriminster,findprisident],\n",
    "    tool_use_behavior='stop_on_first_tool'\n",
    ")\n",
    "\n",
    "resp = await Runner.run(agent, \"who is priminster of pakistan\",max_turns=1)\n",
    "print(resp.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804be92",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - [{'error': {'code': 400, 'message': 'Parallel tool calls are not supported.', 'status': 'INVALID_ARGUMENT'}}]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrisident of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountry\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is asif ali zardari\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m agent = Agent(\n\u001b[32m     14\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     instructions=\u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     tool_use_behavior=\u001b[33m'\u001b[39m\u001b[33mstop_on_first_tool\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m resp = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(agent, \u001b[33m\"\u001b[39m\u001b[33mwho is priminster of pakistan\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(resp.final_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\agents\\run.py:218\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id)\u001b[39m\n\u001b[32m    213\u001b[39m logger.debug(\n\u001b[32m    214\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    215\u001b[39m )\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    219\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_input_guardrails(\n\u001b[32m    220\u001b[39m             starting_agent,\n\u001b[32m    221\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    222\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    223\u001b[39m             copy.deepcopy(\u001b[38;5;28minput\u001b[39m),\n\u001b[32m    224\u001b[39m             context_wrapper,\n\u001b[32m    225\u001b[39m         ),\n\u001b[32m    226\u001b[39m         \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    227\u001b[39m             agent=current_agent,\n\u001b[32m    228\u001b[39m             all_tools=all_tools,\n\u001b[32m    229\u001b[39m             original_input=original_input,\n\u001b[32m    230\u001b[39m             generated_items=generated_items,\n\u001b[32m    231\u001b[39m             hooks=hooks,\n\u001b[32m    232\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    233\u001b[39m             run_config=run_config,\n\u001b[32m    234\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    235\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    236\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    237\u001b[39m         ),\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    240\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._run_single_turn(\n\u001b[32m    241\u001b[39m         agent=current_agent,\n\u001b[32m    242\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    250\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    251\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\agents\\run.py:760\u001b[39m, in \u001b[36mRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    758\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    761\u001b[39m     agent,\n\u001b[32m    762\u001b[39m     system_prompt,\n\u001b[32m    763\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    764\u001b[39m     output_schema,\n\u001b[32m    765\u001b[39m     all_tools,\n\u001b[32m    766\u001b[39m     handoffs,\n\u001b[32m    767\u001b[39m     context_wrapper,\n\u001b[32m    768\u001b[39m     run_config,\n\u001b[32m    769\u001b[39m     tool_use_tracker,\n\u001b[32m    770\u001b[39m     previous_response_id,\n\u001b[32m    771\u001b[39m )\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    774\u001b[39m     agent=agent,\n\u001b[32m    775\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    784\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    785\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\agents\\run.py:919\u001b[39m, in \u001b[36mRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    916\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m    917\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m    920\u001b[39m     system_instructions=system_prompt,\n\u001b[32m    921\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    922\u001b[39m     model_settings=model_settings,\n\u001b[32m    923\u001b[39m     tools=all_tools,\n\u001b[32m    924\u001b[39m     output_schema=output_schema,\n\u001b[32m    925\u001b[39m     handoffs=handoffs,\n\u001b[32m    926\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m    927\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m    928\u001b[39m     ),\n\u001b[32m    929\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    930\u001b[39m )\n\u001b[32m    932\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:61\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     47\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m     previous_response_id: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     55\u001b[39m ) -> ModelResponse:\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m     57\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m     58\u001b[39m         model_config=model_settings.to_json_dict() | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._client.base_url)},\n\u001b[32m     59\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m     60\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     62\u001b[39m             system_instructions,\n\u001b[32m     63\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     64\u001b[39m             model_settings,\n\u001b[32m     65\u001b[39m             tools,\n\u001b[32m     66\u001b[39m             output_schema,\n\u001b[32m     67\u001b[39m             handoffs,\n\u001b[32m     68\u001b[39m             span_generation,\n\u001b[32m     69\u001b[39m             tracing,\n\u001b[32m     70\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     71\u001b[39m         )\n\u001b[32m     73\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m     74\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mReceived model response\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:239\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream)\u001b[39m\n\u001b[32m    233\u001b[39m store = ChatCmplHelpers.get_store_param(\u001b[38;5;28mself\u001b[39m._get_client(), model_settings)\n\u001b[32m    235\u001b[39m stream_options = ChatCmplHelpers.get_stream_options_param(\n\u001b[32m    236\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_client(), model_settings, stream=stream\n\u001b[32m    237\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_client().chat.completions.create(\n\u001b[32m    240\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    241\u001b[39m     messages=converted_messages,\n\u001b[32m    242\u001b[39m     tools=converted_tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    243\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    244\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    245\u001b[39m     frequency_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.frequency_penalty),\n\u001b[32m    246\u001b[39m     presence_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.presence_penalty),\n\u001b[32m    247\u001b[39m     max_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    248\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    249\u001b[39m     response_format=response_format,\n\u001b[32m    250\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    251\u001b[39m     stream=stream,\n\u001b[32m    252\u001b[39m     stream_options=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(stream_options),\n\u001b[32m    253\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(store),\n\u001b[32m    254\u001b[39m     reasoning_effort=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(reasoning_effort),\n\u001b[32m    255\u001b[39m     extra_headers={ **HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {}) },\n\u001b[32m    256\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    257\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    258\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    259\u001b[39m )\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, ChatCompletion):\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2028\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1985\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1986\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1987\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2025\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2026\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2027\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2029\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2030\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2031\u001b[39m             {\n\u001b[32m   2032\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2033\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2034\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2035\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2036\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2037\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2038\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2039\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2040\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2041\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2042\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2043\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2044\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2045\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2046\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2047\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2048\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2049\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2050\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2051\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2052\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2053\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2054\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2055\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2056\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2057\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2058\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2059\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2060\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2061\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2062\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2063\u001b[39m             },\n\u001b[32m   2064\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2065\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2066\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2067\u001b[39m         ),\n\u001b[32m   2068\u001b[39m         options=make_request_options(\n\u001b[32m   2069\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2070\u001b[39m         ),\n\u001b[32m   2071\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2072\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2073\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2074\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_base_client.py:1742\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1728\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1729\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1730\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1737\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1738\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1739\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1740\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1741\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arman\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\openai\\_base_client.py:1549\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1546\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1548\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1549\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1553\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - [{'error': {'code': 400, 'message': 'Parallel tool calls are not supported.', 'status': 'INVALID_ARGUMENT'}}]"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled,ModelSettings,function_tool\n",
    "\n",
    "set_tracing_disabled(disabled=False)\n",
    "@function_tool\n",
    "def findpriminster(country:str):\n",
    "    print('tools calls')\n",
    "    return f\"Priminister of {country} is Imran Khan\"\n",
    "\n",
    "@function_tool\n",
    "def findprisident(country:str):\n",
    "    print('tools calls prisident')\n",
    "    return f\"Prisident of {country} is asif ali zardari\"\n",
    "agent = Agent(\n",
    "    name=\"\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    model=model,\n",
    "    model_settings=ModelSettings(tool_choice='findprisident',\n",
    "    parallel_tool_calls=True),\n",
    "    tools=[findpriminster,findprisident],\n",
    "    tool_use_behavior='stop_on_first_tool'\n",
    ")\n",
    "\n",
    "resp = await Runner.run(agent, \"who is priminster of pakistan\")\n",
    "print(resp.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c403a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tools calls\n",
      "Priminister of pakistan is Imran Khan\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, set_tracing_disabled,ModelSettings,function_tool\n",
    "\n",
    "set_tracing_disabled(disabled=False)\n",
    "@function_tool\n",
    "def findpriminster(country:str):\n",
    "    print('tools calls')\n",
    "    return f\"Priminister of {country} is Imran Khan\"\n",
    "\n",
    "@function_tool\n",
    "def findprisident(country:str):\n",
    "    print('tools calls prisident')\n",
    "    return f\"Prisident of {country} is asif ali zardari\"\n",
    "agent = Agent(\n",
    "    name=\"\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    model=model,\n",
    "    model_settings=ModelSettings(),\n",
    "    tools=[findpriminster,findprisident],\n",
    "    tool_use_behavior='stop_on_first_tool'\n",
    ")\n",
    "\n",
    "resp = await Runner.run(agent, \"who is priminster of pakistan\")\n",
    "print(resp.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad24b16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Output: The user has repeated the phrase \"This is one chunk.\" many, many times.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, ModelSettings\n",
    "import asyncio\n",
    "\n",
    "# Create a very long input (simulate overflow)\n",
    "long_text = \"This is one chunk. \" * 50000  # Creates >100,000 tokens\n",
    "\n",
    "# Define a simple agent\n",
    "agent = Agent(\n",
    "    name=\"truncate_demo_agent\",\n",
    "    instructions=\"Summarize the user's message.\",\n",
    "    model=model,\n",
    "    model_settings=ModelSettings(\n",
    "        truncation=\"auto\",  \n",
    "    ),\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    response = await Runner.run(agent, input=long_text)\n",
    "    print(\"✅ Output:\", response.final_output)\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7fed8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe17f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
