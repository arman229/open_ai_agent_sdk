{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6dcc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply() \n",
    "import os\n",
    "from agents import Agent, OpenAIChatCompletionsModel, AsyncOpenAI, Runner\n",
    "import asyncio\n",
    "try:\n",
    "\n",
    "    GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not GEMINI_API_KEY:\n",
    "        raise ValueError(\"GEMINI ApI key not found\")\n",
    "  \n",
    "    external_client = AsyncOpenAI(\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "        api_key=GEMINI_API_KEY,\n",
    "    )\n",
    "    model = OpenAIChatCompletionsModel(\n",
    "        model=\"gemini-2.0-flash\", openai_client=external_client\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf696c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run starting ===\n",
      "[Event]:AgentUpdatedStreamEvent(new_agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x00000131433C18A0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x00000131433A0D70>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1752839359.439591, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{}', item_id='__fake_id__', output_index=0, sequence_number=2, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=3, type='response.output_item.done'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1752839359.439591, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None)], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=4, type='response.completed'), type='raw_response_event')\n",
      "\n",
      "[Event]:RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x00000131433C18A0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x00000131433A0D70>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), type='tool_call_item'), type='run_item_stream_event')\n",
      "\n",
      "[Event]:RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x00000131433C18A0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x00000131433A0D70>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': '', 'output': '7', 'type': 'function_call_output'}, output=7, type='tool_call_output_item'), type='run_item_stream_event')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Event]:RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1752839360.6006618, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=None), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='I', item_id='__fake_id__', output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' am programmed to tell 7 jokes.\\n', item_id='__fake_id__', output_index=0, sequence_number=4, type='response.output_text.delta'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='I am programmed to tell 7 jokes.\\n', type='output_text', logprobs=None), sequence_number=5, type='response.content_part.done'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='I am programmed to tell 7 jokes.\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), output_index=0, sequence_number=6, type='response.output_item.done'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1752839360.6006618, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='I am programmed to tell 7 jokes.\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=7, type='response.completed'), type='raw_response_event')\n",
      "\n",
      "[Event]:RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x00000131433C18A0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x00000131433A0D70>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='I am programmed to tell 7 jokes.\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n",
      "\n",
      "=== Run complete ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n",
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from agents import Agent, ItemHelpers, Runner, function_tool\n",
    "\n",
    "@function_tool\n",
    "def how_many_jokes() -> int:\n",
    "    return random.randint(1, 10)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
    "        tools=[how_many_jokes],\n",
    "        model=model\n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(\n",
    "        agent,\n",
    "        input=\"Hello\",\n",
    "    )\n",
    "    print(\"=== Run starting ===\")\n",
    "\n",
    "    async for event in result.stream_events(): \n",
    "        print(f\"[Event]:{event}\\n\")\n",
    "\n",
    "    print(\"=== Run complete ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2afeccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run starting ===\n",
      "[Event]:AgentUpdatedStreamEvent(new_agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001B5323C9EE0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001B534064C20>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1752669931.201525, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{}', item_id='__fake_id__', output_index=0, sequence_number=2, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=3, type='response.output_item.done'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1752669931.201525, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None)], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=4, type='response.completed'), type='raw_response_event')\n",
      "\n",
      "[Event]:RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001B5323C9EE0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001B534064C20>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), type='tool_call_item'), type='run_item_stream_event')\n",
      "\n",
      "[Event]:RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001B5323C9EE0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001B534064C20>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': '', 'output': 'An error occurred while running the tool. Please try again. Error: Errors not generate how many jokes ', 'type': 'function_call_output'}, output='An error occurred while running the tool. Please try again. Error: Errors not generate how many jokes ', type='tool_call_output_item'), type='run_item_stream_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1752669935.490333, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=None), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='I am sorry,', item_id='__fake_id__', output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' I cannot fulfill this request. There was an error when calling the tool. Please try again.', item_id='__fake_id__', output_index=0, sequence_number=4, type='response.output_text.delta'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='\\n', item_id='__fake_id__', output_index=0, sequence_number=5, type='response.output_text.delta'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='I am sorry, I cannot fulfill this request. There was an error when calling the tool. Please try again.\\n', type='output_text', logprobs=None), sequence_number=6, type='response.content_part.done'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='I am sorry, I cannot fulfill this request. There was an error when calling the tool. Please try again.\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), output_index=0, sequence_number=7, type='response.output_item.done'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1752669935.490333, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='I am sorry, I cannot fulfill this request. There was an error when calling the tool. Please try again.\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=8, type='response.completed'), type='raw_response_event')\n",
      "\n",
      "[Event]:RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001B5323C9EE0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001B534064C20>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='I am sorry, I cannot fulfill this request. There was an error when calling the tool. Please try again.\\n', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n",
      "\n",
      "=== Run complete ===\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from agents import Agent, ItemHelpers, Runner, function_tool\n",
    "\n",
    "@function_tool\n",
    "def how_many_jokes() -> int:\n",
    "    raise ValueError('Errors not generate how many jokes ')\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
    "        tools=[how_many_jokes],\n",
    "        model=model\n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(\n",
    "        agent,\n",
    "        input=\"Hello\",\n",
    "    )\n",
    "    print(\"=== Run starting ===\")\n",
    "\n",
    "    async for event in result.stream_events(): \n",
    "        print(f\"[Event]:{event}\\n\")\n",
    "\n",
    "    print(\"=== Run complete ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e47a1beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run starting ===\n",
      "[Event]:AgentUpdatedStreamEvent(new_agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001B5323CA2A0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001B534064C20>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1752670044.9285226, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{}', item_id='__fake_id__', output_index=0, sequence_number=2, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=3, type='response.output_item.done'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1752670044.9285226, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None)], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=4, type='response.completed'), type='raw_response_event')\n",
      "\n",
      "[Event]:RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001B5323CA2A0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001B534064C20>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), type='tool_call_item'), type='run_item_stream_event')\n",
      "\n",
      "[Event]:RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001B5323CA2A0>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001B534064C20>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': '', 'output': 'An error occurred while running the tool. Please try again. Error: Errors not generate how many jokes ', 'type': 'function_call_output'}, output='An error occurred while running the tool. Please try again. Error: Errors not generate how many jokes ', type='tool_call_output_item'), type='run_item_stream_event')\n",
      "\n"
     ]
    },
    {
     "ename": "MaxTurnsExceeded",
     "evalue": "Max turns (1) exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMaxTurnsExceeded\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Run complete ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     19\u001b[39m result = Runner.run_streamed(\n\u001b[32m     20\u001b[39m     agent,\n\u001b[32m     21\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mHello\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m     max_turns=\u001b[32m1\u001b[39m\n\u001b[32m     23\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Run starting ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m result.stream_events(): \n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Event]:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Run complete ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\result.py:215\u001b[39m, in \u001b[36mRunResultStreaming.stream_events\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m._cleanup_tasks()\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception\n",
      "\u001b[31mMaxTurnsExceeded\u001b[39m: Max turns (1) exceeded"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from agents import Agent, ItemHelpers, Runner, function_tool\n",
    "\n",
    "@function_tool\n",
    "def how_many_jokes() -> int:\n",
    "    raise ValueError('Errors not generate how many jokes ')\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
    "        tools=[how_many_jokes],\n",
    "        model=model\n",
    "        \n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(\n",
    "        agent,\n",
    "        input=\"Hello\",\n",
    "        max_turns=1\n",
    "    )\n",
    "    print(\"=== Run starting ===\")\n",
    "\n",
    "    async for event in result.stream_events(): \n",
    "        print(f\"[Event]:{event}\\n\")\n",
    "\n",
    "    print(\"=== Run complete ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184ffee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run starting ===\n",
      "[Event]:AgentUpdatedStreamEvent(new_agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001B534FFD300>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001B534064C20>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1752673009.5368915, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseFunctionCallArgumentsDeltaEvent(delta='{}', item_id='__fake_id__', output_index=0, sequence_number=2, type='response.function_call_arguments.delta'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), output_index=0, sequence_number=3, type='response.output_item.done'), type='raw_response_event')\n",
      "\n",
      "[Event]:RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1752673009.5368915, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None)], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=4, type='response.completed'), type='raw_response_event')\n",
      "\n",
      "[Event]:RunItemStreamEvent(name='tool_called', item=ToolCallItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001B534FFD300>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001B534064C20>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseFunctionToolCall(arguments='{}', call_id='', name='how_many_jokes', type='function_call', id='__fake_id__', status=None), type='tool_call_item'), type='run_item_stream_event')\n",
      "\n",
      "[Event]:RunItemStreamEvent(name='tool_output', item=ToolCallOutputItem(agent=Agent(name='Joker', handoff_description=None, tools=[FunctionTool(name='how_many_jokes', description='', params_json_schema={'properties': {}, 'title': 'how_many_jokes_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x000001B534FFD300>, strict_json_schema=True, is_enabled=True)], mcp_servers=[], mcp_config={}, instructions='First call the `how_many_jokes` tool, then tell that many jokes.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001B534064C20>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item={'call_id': '', 'output': '4', 'type': 'function_call_output'}, output=4, type='tool_call_output_item'), type='run_item_stream_event')\n",
      "\n"
     ]
    },
    {
     "ename": "MaxTurnsExceeded",
     "evalue": "Max turns (1) exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMaxTurnsExceeded\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Run complete ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     20\u001b[39m result = Runner.run_streamed(\n\u001b[32m     21\u001b[39m     agent,\n\u001b[32m     22\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mHello\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     max_turns=\u001b[32m1\u001b[39m\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Run starting ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m result.stream_events(): \n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Event]:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Run complete ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\result.py:215\u001b[39m, in \u001b[36mRunResultStreaming.stream_events\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m._cleanup_tasks()\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stored_exception\n",
      "\u001b[31mMaxTurnsExceeded\u001b[39m: Max turns (1) exceeded"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from agents import Agent, ItemHelpers, Runner, function_tool\n",
    "import time\n",
    "@function_tool\n",
    "def how_many_jokes() -> int:\n",
    "    time.sleep(129)\n",
    "    return 4\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
    "        tools=[how_many_jokes],\n",
    "        model=model\n",
    "        \n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(\n",
    "        agent,\n",
    "        input=\"Hello\",\n",
    "        max_turns=1\n",
    "    )\n",
    "    print(\"=== Run starting ===\")\n",
    "\n",
    "    async for event in result.stream_events(): \n",
    "        print(f\"[Event]:{event}\\n\")\n",
    "\n",
    "    print(\"=== Run complete ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3afc0106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run starting ===\n",
      "Agent updated: Joker\n",
      "-- Message output:\n",
      " ## The Poet-Philosopher: A Glimpse into the Life and Thought of Allama Iqbal\n",
      "\n",
      "Allama Muhammad Iqbal, often referred to as the \"Poet of the East,\" was a towering figure in the intellectual and political landscape of the Indian subcontinent during the early 20th century. His poetry, philosophy, and political thought profoundly influenced the movement for a separate homeland for Muslims of India, ultimately leading to the creation of Pakistan. But beyond his political legacy, Iqbal's enduring appeal lies in his ability to connect with the human spirit, inspiring generations with his message of self-reliance, spiritual awakening, and the potential for individual and collective progress.\n",
      "\n",
      "Born in Sialkot, Punjab (now in Pakistan) in 1877, Iqbal received a traditional Islamic education, emphasizing Arabic and Persian languages and literature. He later pursued higher education, earning degrees in philosophy from Government College, Lahore, and eventually a doctorate from the University of Munich. His academic journey exposed him to Western philosophical\n",
      "=== Run complete ===\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from agents import Agent, ItemHelpers, Runner, function_tool\n",
    "import time\n",
    "@function_tool\n",
    "def how_many_jokes() -> int:\n",
    "    time.sleep(12)\n",
    "    return 4\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "      \n",
    "       \n",
    "        model=model\n",
    "        \n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(\n",
    "        agent,\n",
    "        input=\"writ about easy on allama iqbal in 500 words\",\n",
    "        max_turns=2\n",
    "    )\n",
    "    print(\"=== Run starting ===\")\n",
    "\n",
    "    async for event in result.stream_events():\n",
    "        # We'll ignore the raw responses event deltas\n",
    "        if event.type == \"raw_response_event\":\n",
    "            continue\n",
    "        # When the agent updates, print that\n",
    "        elif event.type == \"agent_updated_stream_event\":\n",
    "            print(f\"Agent updated: {event.new_agent.name}\")\n",
    "            continue\n",
    "        # When items are generated, print them\n",
    "        elif event.type == \"run_item_stream_event\":\n",
    "            if event.item.type == \"tool_call_item\":\n",
    "                print(\"-- Tool was called\")\n",
    "            elif event.item.type == \"tool_call_output_item\":\n",
    "                print(f\"-- Tool output: {event.item.output}\")\n",
    "            elif event.item.type == \"message_output_item\":\n",
    "                print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n",
    "            else:\n",
    "                pass  # Ignore other event types\n",
    "\n",
    "    print(\"=== Run complete ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85511bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run starting ===\n",
      "## Allama Iqbal: A Poet of the East and a Visionary of Hope\n",
      "\n",
      "Allama Muhammad Iqbal, often hailed as the \"Poet of the East\" and \"National Poet of Pakistan,\" was a towering figure in the intellectual and political landscape of the 20th century. He was a philosopher, poet, barrister, and politician whose ideas deeply influenced the Pakistan Movement and continue to resonate with millions today. His legacy lies not just in his captivating poetry but also in his profound insights into the spiritual, social, and political challenges facing the Muslim world.\n",
      "\n",
      "Born in Sialkot, British India, in 1877, Iqbal received a traditional Islamic education alongside modern schooling. He pursued higher education in Lahore, Cambridge, Munich, and Heidelberg, earning degrees in philosophy and law. This exposure to both Eastern and Western thought shaped his intellectual development and allowed him to critically analyze the strengths and weaknesses of both traditions.\n",
      "\n",
      "Iqbal's poetry, primarily written in Urdu and Persian, is rich with symbolism, philosophical depth, and passionate calls for self-awareness and spiritual awakening. He was deeply concerned with the decline of the Muslim world, attributing it to a disconnect from its own rich intellectual and spiritual heritage. He urged Muslims to rediscover their identity, embrace dynamism, and cultivate a spirit of self-reliance.\n",
      "\n",
      "One of Iqbal's most significant contributions was his concept of \"Khudi,\" often translated as \"selfhood\" or \"ego.\" He believed that each individual possesses a unique potential that must be realized through continuous effort and self-affirmation. This concept was a powerful antidote to the prevailing sense of apathy and fatalism that he observed in Muslim society. He argued that by developing their \"Khudi,\" individuals could contribute to the collective strength and progress of the community.\n",
      "\n",
      "His famous poem, \"Shikwa\" (Complaint) and its response \"Jawab-e-Shikwa\" (Answer to the Complaint), are poignant dialogues between the poet and God, expressing the grievances of the Muslim community and receiving a divine response urging them to revitalize their faith and work towards self-improvement. These poems capture the essence of Iqbal's philosophy  a deep love for Islam coupled with a critical examination of its followers' shortcomings.\n",
      "\n",
      "While initially advocating for Hindu-Muslim unity, Iqbal gradually became convinced that the diverse religious and cultural identities within India necessitated a separate homeland for Muslims to preserve their distinct identity and culture. In his 1930 Allahabad address, he famously proposed the creation of a separate state in northwestern India for Muslims, an idea that became the cornerstone of the Pakistan Movement. This vision, while controversial then, ultimately paved the way for the creation of Pakistan in 1947.\n",
      "\n",
      "Iqbal's influence extends far beyond the borders of Pakistan. His poetry and philosophical ideas have inspired movements for social justice, self-determination, and spiritual renewal across the Muslim world. He remains a symbol of intellectual courage, spiritual awakening, and the power of poetry to inspire change. He died in 1938, leaving behind a rich legacy that continues to inspire generations to strive for a better future rooted in faith, self-awareness, and a commitment to social justice. His message of hope and self-reliance remains relevant in today's world, making him a timeless figure in the history of ideas.\n",
      "## Allama Iqbal: A Poet of the East and a Visionary of Hope\n",
      "\n",
      "Allama Muhammad Iqbal, often hailed as the \"Poet of the East\" and \"National Poet of Pakistan,\" was a towering figure in the intellectual and political landscape of the 20th century. He was a philosopher, poet, barrister, and politician whose ideas deeply influenced the Pakistan Movement and continue to resonate with millions today. His legacy lies not just in his captivating poetry but also in his profound insights into the spiritual, social, and political challenges facing the Muslim world.\n",
      "\n",
      "Born in Sialkot, British India, in 1877, Iqbal received a traditional Islamic education alongside modern schooling. He pursued higher education in Lahore, Cambridge, Munich, and Heidelberg, earning degrees in philosophy and law. This exposure to both Eastern and Western thought shaped his intellectual development and allowed him to critically analyze the strengths and weaknesses of both traditions.\n",
      "\n",
      "Iqbal's poetry, primarily written in Urdu and Persian, is rich with symbolism, philosophical depth, and passionate calls for self-awareness and spiritual awakening. He was deeply concerned with the decline of the Muslim world, attributing it to a disconnect from its own rich intellectual and spiritual heritage. He urged Muslims to rediscover their identity, embrace dynamism, and cultivate a spirit of self-reliance.\n",
      "\n",
      "One of Iqbal's most significant contributions was his concept of \"Khudi,\" often translated as \"selfhood\" or \"ego.\" He believed that each individual possesses a unique potential that must be realized through continuous effort and self-affirmation. This concept was a powerful antidote to the prevailing sense of apathy and fatalism that he observed in Muslim society. He argued that by developing their \"Khudi,\" individuals could contribute to the collective strength and progress of the community.\n",
      "\n",
      "His famous poem, \"Shikwa\" (Complaint) and its response \"Jawab-e-Shikwa\" (Answer to the Complaint), are poignant dialogues between the poet and God, expressing the grievances of the Muslim community and receiving a divine response urging them to revitalize their faith and work towards self-improvement. These poems capture the essence of Iqbal's philosophy  a deep love for Islam coupled with a critical examination of its followers' shortcomings.\n",
      "\n",
      "While initially advocating for Hindu-Muslim unity, Iqbal gradually became convinced that the diverse religious and cultural identities within India necessitated a separate homeland for Muslims to preserve their distinct identity and culture. In his 1930 Allahabad address, he famously proposed the creation of a separate state in northwestern India for Muslims, an idea that became the cornerstone of the Pakistan Movement. This vision, while controversial then, ultimately paved the way for the creation of Pakistan in 1947.\n",
      "\n",
      "Iqbal's influence extends far beyond the borders of Pakistan. His poetry and philosophical ideas have inspired movements for social justice, self-determination, and spiritual renewal across the Muslim world. He remains a symbol of intellectual courage, spiritual awakening, and the power of poetry to inspire change. He died in 1938, leaving behind a rich legacy that continues to inspire generations to strive for a better future rooted in faith, self-awareness, and a commitment to social justice. His message of hope and self-reliance remains relevant in today's world, making him a timeless figure in the history of ideas.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from agents import Agent, ItemHelpers, Runner, function_tool\n",
    "import time\n",
    "import asyncio\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "@function_tool\n",
    "def how_many_jokes() -> int:\n",
    "    time.sleep(12)\n",
    "    return 4\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "      \n",
    "       \n",
    "        model=model\n",
    "        \n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(\n",
    "        agent,\n",
    "        input=\"writ about easy on allama iqbal in 500 words\",\n",
    "        max_turns=2\n",
    "    )\n",
    "    print(\"=== Run starting ===\")\n",
    "\n",
    "    async for event in result.stream_events():\n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "    print(result.final_output)\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4548ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run starting ===\n",
      "## The Poet of the East: An Easy Look at Allama Iqbal\n",
      "\n",
      "Allama Muhammad Iqbal, often referred to as the \"Poet of the East\" or \"Philosopher of Pakistan,\" was a towering figure of the 20th century. He was a poet, philosopher, lawyer, and politician whose ideas and verses deeply influenced the Pakistan Movement and continue to resonate today. While his philosophical ideas are complex, his impact and underlying message are accessible and inspiring.\n",
      "\n",
      "Born in Sialkot, British India, in 1877, Iqbal received his early education in a traditional Islamic setting. He later studied at Government College Lahore, Cambridge University, and Munich## The Poet of the East: An Easy Look at Allama Iqbal\n",
      "\n",
      "Allama Muhammad Iqbal, often referred to as the \"Poet of the East\" or \"Philosopher of Pakistan,\" was a towering figure of the 20th century. He was a poet, philosopher, lawyer, and politician whose ideas and verses deeply influenced the Pakistan Movement and continue to resonate today. While his philosophical ideas are complex, his impact and underlying message are accessible and inspiring.\n",
      "\n",
      "Born in Sialkot, British India, in 1877, Iqbal received his early education in a traditional Islamic setting. He later studied at Government College Lahore, Cambridge University, and Munich\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from agents import Agent, ItemHelpers, Runner, function_tool\n",
    "import time\n",
    "import asyncio\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "@function_tool\n",
    "def how_many_jokes() -> int:\n",
    "    time.sleep(12)\n",
    "    return 4\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "      \n",
    "       \n",
    "        model=model\n",
    "        \n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(\n",
    "        agent,\n",
    "        input=\"writ about easy on allama iqbal in 500 words\",\n",
    "        max_turns=1\n",
    "    )\n",
    "    print(\"=== Run starting ===\")\n",
    "\n",
    "    async for event in result.stream_events():\n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "    print(result.final_output)\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92fc15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Run starting ===\n",
      "## Allama Iqbal: A Poet, Philosopher, and Visionary of Pakistan\n",
      "\n",
      "Allama Muhammad Iqbal, often hailed as the \"Poet of the East,\" was a towering figure in the intellectual and political landscape of the Indian subcontinent during the late 19th and early 20th centuries. He was a poet, philosopher, barrister, and politician whose ideas deeply influenced the Pakistan Movement and ultimately led to the creation of Pakistan.\n",
      "\n",
      "Iqbal was born in Sialkot, Punjab, in 1877. He received his early education in a traditional Islamic environment, learning Arabic and Persian, which instilled in him a deep appreciation for Islamic literature and philosophy. He later graduated from Government College, Lahore, and went on to pursue higher education in Europe, studying philosophy and law at Cambridge University, Munich University, and Lincoln's Inn. His experiences abroad exposed him to Western philosophical thought and sharpened his understanding of both Eastern and Western civilizations.\n",
      "\n",
      "Iqbal's genius lies in his ability"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from agents import Agent, ItemHelpers, Runner, function_tool\n",
    "import time\n",
    "import asyncio\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "@function_tool\n",
    "def how_many_jokes() -> int:\n",
    "    time.sleep(12)\n",
    "    return 4\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "      \n",
    "       \n",
    "        model=model\n",
    "        \n",
    "    )\n",
    "\n",
    "    result = Runner.run_streamed(\n",
    "        agent,\n",
    "        input=\"writ about easy on allama iqbal in 500 words\",\n",
    "        max_turns=1\n",
    "    )\n",
    "    print(\"=== Run starting ===\")\n",
    "\n",
    "    async for event in result.stream_events():\n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            print(event.data.delta, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "    print(result.final_output)\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeae7e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun calling\n",
      "before 4\n",
      "=== Run complete ===\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from agents import Agent, ItemHelpers, Runner, function_tool\n",
    "import time\n",
    "@function_tool\n",
    "def how_many_jokes() -> int:\n",
    "    print('fun calling')\n",
    "    time.sleep(4)\n",
    "    print('before 4')\n",
    "    return 4\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
    "        tools=[how_many_jokes],\n",
    "        model=model\n",
    "        \n",
    "    )\n",
    "\n",
    "    result = Runner.run_sync(\n",
    "        agent,\n",
    "        input=\"Hello\",\n",
    "        max_turns=2\n",
    "    ) \n",
    "\n",
    "    print(\"=== Run complete ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fb0b817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fun calling\n",
      "before 4\n",
      "=== Run complete ===\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from agents import Agent, ItemHelpers, Runner, function_tool\n",
    "import time\n",
    "@function_tool\n",
    "async def how_many_jokes() -> int:\n",
    "    print('fun calling')\n",
    "    await asyncio.sleep(10)\n",
    "    print('before 4')\n",
    "    return 4\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
    "        tools=[how_many_jokes],\n",
    "        model=model\n",
    "        \n",
    "    )\n",
    "\n",
    "    result = Runner.run_sync(\n",
    "        agent,\n",
    "        input=\"Hello\",\n",
    "        max_turns=2\n",
    "    ) \n",
    "\n",
    "    print(\"=== Run complete ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba40528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
