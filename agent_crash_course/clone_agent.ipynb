{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4012d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "# 1. AGENT LEVEL\n",
    "import os\n",
    "from agents import Agent, OpenAIChatCompletionsModel, AsyncOpenAI, Runner\n",
    "import asyncio\n",
    "try:\n",
    "\n",
    "    GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not GEMINI_API_KEY:\n",
    "        raise ValueError(\"GEMINI ApI key not found\")\n",
    "  \n",
    "    external_client = AsyncOpenAI(\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "        api_key=GEMINI_API_KEY,\n",
    "    )\n",
    "    model = OpenAIChatCompletionsModel(\n",
    "        model=\"gemini-1.5-flash\", openai_client=external_client\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a67598e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here are 5 jokes for you:\n",
      "\n",
      "1.  Why don't scientists trust atoms?\n",
      "    Because they make up everything!\n",
      "\n",
      "2.  Why did the scarecrow win an award?\n",
      "    Because he was outstanding in his field!\n",
      "\n",
      "3.  What do you call a lazy kangaroo?\n",
      "    Pouch potato!\n",
      "\n",
      "4.  Why did the bicycle fall over?\n",
      "    Because it was two tired!\n",
      "\n",
      "5.  What musical instrument is found in the bathroom?\n",
      "    A tuba toothpaste!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio \n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "from agents import Agent\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "class OutputTypes(BaseModel):\n",
    "    jokes:List[str]\n",
    "old_agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        model=model,\n",
    "        # output_type=OutputTypes\n",
    "       \n",
    "    )\n",
    "async def main():\n",
    "    \n",
    "\n",
    "    result = Runner.run_sync(old_agent, input=\"Please tell me 5 jokes.\")\n",
    "    print(result.final_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f622730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunResult:\n",
      "- Last agent: Agent(name=\"new_agent\", ...)\n",
      "- Final output (OutputTypes):\n",
      "    {\n",
      "      \"jokes\": [\n",
      "        \"استاد: کل تم سکول کیوں نہیں آئے؟ شاگرد: امی نے کہا تھا آج کوئی خاص کام نہیں ہے۔\",\n",
      "        \"ایک آدمی دوسرے سے: یار میں نے سنا ہے تم نے شادی کر لی؟ دوسرا: ہاں یار، مصیبت میں پھنس گیا ہوں۔ پہلا: کیوں بھئی؟ دوسرا: میری بیوی میکے چلی گئی ہے اور کہہ گئی ہے جب تک نیا فرنیچر نہیں بنواؤ گے واپس نہیں آؤں گی۔ پہلا: اوہ، یہ تو واقعی مصیبت ہے۔ دوسرا: مصیبت یہ نہیں ہے، مصیبت یہ ہے کہ میں نے سارے پیسے تو اسے میکے چھوڑنے میں خرچ کر دئیے!\",\n",
      "        \"جج: تم نے جرم کیا ہے؟ ملزم: میں نے ایک دکان سے ڈبل روٹی چرائی ہے۔ جج: کیوں چرائی؟ تمہیں بھوک لگی ہوگی؟ ملزم: ہاں جناب۔ جج: تو تم نے دکان دار سے مانگ کیوں نہیں لی؟ ملزم: مانگی تھی، اس نے دینے سے انکار کر دیا تھا۔ جج: اچھا، تو تم نے اس سے پہلے کبھی ڈبل روٹی نہیں چرائی؟ ملزم: پہلے بھی چرائی تھی، لیکن اس وقت میری جیب میں پیسے تھے۔\",\n",
      "        \"ایک شخص اپنے دوست سے: یار کل رات میں نے خواب میں دیکھا کہ میں پاکستان کا صدر بن گیا ہوں۔ دوست: اوہ، پھر کیا ہوا؟ شخص: پھر کیا، صبح میری بیوی نے اٹھا دیا۔\",\n",
      "        \"شوہر: یہ جو تم ہر بات میں اپنی امی امی کرتی ہو، تمہاری امی نے تمہیں کچھ سکھایا بھی ہے یا نہیں؟ بیوی: سکھایا ہے! یہی کہ ہر بات میں اپنے شوہر کی سنو۔\"\n",
      "      ]\n",
      "    }\n",
      "- 1 new item(s)\n",
      "- 1 raw response(s)\n",
      "- 0 input guardrail result(s)\n",
      "- 0 output guardrail result(s)\n",
      "(See `RunResult` for more details)\n",
      "jokes=['استاد: کل تم سکول کیوں نہیں آئے؟ شاگرد: امی نے کہا تھا آج کوئی خاص کام نہیں ہے۔', 'ایک آدمی دوسرے سے: یار میں نے سنا ہے تم نے شادی کر لی؟ دوسرا: ہاں یار، مصیبت میں پھنس گیا ہوں۔ پہلا: کیوں بھئی؟ دوسرا: میری بیوی میکے چلی گئی ہے اور کہہ گئی ہے جب تک نیا فرنیچر نہیں بنواؤ گے واپس نہیں آؤں گی۔ پہلا: اوہ، یہ تو واقعی مصیبت ہے۔ دوسرا: مصیبت یہ نہیں ہے، مصیبت یہ ہے کہ میں نے سارے پیسے تو اسے میکے چھوڑنے میں خرچ کر دئیے!', 'جج: تم نے جرم کیا ہے؟ ملزم: میں نے ایک دکان سے ڈبل روٹی چرائی ہے۔ جج: کیوں چرائی؟ تمہیں بھوک لگی ہوگی؟ ملزم: ہاں جناب۔ جج: تو تم نے دکان دار سے مانگ کیوں نہیں لی؟ ملزم: مانگی تھی، اس نے دینے سے انکار کر دیا تھا۔ جج: اچھا، تو تم نے اس سے پہلے کبھی ڈبل روٹی نہیں چرائی؟ ملزم: پہلے بھی چرائی تھی، لیکن اس وقت میری جیب میں پیسے تھے۔', 'ایک شخص اپنے دوست سے: یار کل رات میں نے خواب میں دیکھا کہ میں پاکستان کا صدر بن گیا ہوں۔ دوست: اوہ، پھر کیا ہوا؟ شخص: پھر کیا، صبح میری بیوی نے اٹھا دیا۔', 'شوہر: یہ جو تم ہر بات میں اپنی امی امی کرتی ہو، تمہاری امی نے تمہیں کچھ سکھایا بھی ہے یا نہیں؟ بیوی: سکھایا ہے! یہی کہ ہر بات میں اپنے شوہر کی سنو۔']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from openai import BaseModel\n",
    "from typing import List\n",
    "from agents import AgentOutputSchema\n",
    " \n",
    "new_agent = old_agent.clone(\n",
    "    name='new_agent',\n",
    "    instructions='you are an helpful assistant response in urdu',\n",
    "    output_type=OutputTypes, \n",
    "    output_type=AgentOutputSchema(OutputTypes, strict_json_schema=False)\n",
    ")\n",
    "result = Runner.run_sync(new_agent, input=\"Please tell me 5 jokes.\")\n",
    "print(result)\n",
    "print(result.final_output)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90667ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here are 5 jokes for you:\n",
      "\n",
      "1.  Why don't scientists trust atoms?\n",
      "    Because they make up everything!\n",
      "\n",
      "2.  Why did the scarecrow win an award?\n",
      "    Because he was outstanding in his field!\n",
      "\n",
      "3.  What do you call a lazy kangaroo?\n",
      "    Pouch potato!\n",
      "\n",
      "4.  Why did the bicycle fall over?\n",
      "    Because it was two tired!\n",
      "\n",
      "5.  What musical instrument is found in the bathroom?\n",
      "    A tuba toothpaste!\n",
      "\n",
      "*******************\n",
      "jokes=['بیوی: سنو جی، آج ہماری شادی کی سالگرہ ہے۔ شوہر: مبارک ہو، کیا پروگرام ہے؟ بیوی: پروگرام یہ ہے کہ آج میں جو کہوں گی، تمہیں وہ کرنا پڑے گا۔ شوہر: ٹھیک ہے، میں تیار ہوں۔ بیوی: تو پھر چلو برتن دھو!', 'استاد: کل تم سکول کیوں نہیں آئے تھے؟ شاگرد: امی نے کہا تھا آج پیڑ لگانے جانا ہے۔ استاد: تو اس میں سکول نہ آنے کی کیا بات تھی؟ شاگرد: سارے پیڑ تو میں نے ہی لگانے تھے۔', 'ایک آدمی ایک ریستوران میں گیا اور ویٹر سے پوچھا: آپ کے پاس مینڈک کا سوپ ہے؟ ویٹر نے کہا: ہاں ہے۔ آدمی نے کہا: تو پھر ایک لے آؤ، لیکن اس میں مکھی نہ ہو۔ ویٹر نے سوپ لے آیا اور آدمی نے اس میں مکھی پائی۔ آدمی نے ویٹر کو بلایا اور کہا: میں نے کہا تھا اس میں مکھی نہیں ہونی چاہیے! ویٹر نے جواب دیا: سر، ہم نے مکھی نکال دی تھی، لیکن وہ واپس کود گئی۔', 'باپ: بیٹا، تم نے سائنس کی کتاب سے چیٹنگ کیوں کی؟ بیٹا: ابا، سائنس خود کہتی ہے کہ نقل کے لیے بھی عقل کی ضرورت ہوتی ہے۔', 'ایک کنجوس آدمی مر گیا۔ اس کی بیوی نے قبر پر لکھا: \"دیکھو، اب کتنا سیدھا لیٹا ہے۔\"']\n",
      "*******************\n",
      "Why don't scientists trust atoms? \n",
      "\n",
      "Because they make up everything!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from openai import BaseModel\n",
    "from typing import List\n",
    "from agents import AgentOutputSchema\n",
    "import asyncio \n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "from agents import Agent\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "class OutputTypes(BaseModel):\n",
    "    jokes:List[str]\n",
    "    \n",
    "old_agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        model=model,\n",
    "        \n",
    "       \n",
    "    )\n",
    "\n",
    "result = Runner.run_sync(old_agent, input=\"Please tell me 5 jokes.\")\n",
    "print(result.final_output)\n",
    "print('*******************')\n",
    "new_agent = old_agent.clone(\n",
    "    name='new_agent',\n",
    "    instructions='you are an helpful assistant response in urdu',\n",
    " \n",
    "    output_type=AgentOutputSchema(OutputTypes, strict_json_schema=False)\n",
    ")\n",
    "result = Runner.run_sync(new_agent, input=\"Please tell me 5 jokes.\")\n",
    " \n",
    "print(result.final_output)\n",
    "print('*******************')\n",
    "result = Runner.run_sync(old_agent, input=\"Please tell me 1 jokes.\")\n",
    "print(result.final_output)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "176beb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, here are 5 jokes for you:\n",
      "\n",
      "1.  Why don't scientists trust atoms?\n",
      "    Because they make up everything!\n",
      "\n",
      "2.  Why did the bicycle fall over?\n",
      "    Because it was two tired!\n",
      "\n",
      "3.  What do you call a lazy kangaroo?\n",
      "    Pouch potato!\n",
      "\n",
      "4.  Why did the scarecrow win an award?\n",
      "    Because he was outstanding in his field!\n",
      "\n",
      "5.  Parallel lines have so much in common. It's a shame they'll never meet.\n",
      "\n",
      "*******************\n",
      "Bilkul! Yeh rahein paanch mazahiya jumlay (jokes):\n",
      "\n",
      "1.  **Ustaad (teacher):** Woh kaunsi cheez hai jo tumhare paas jitni zyada hogi, utni hi kam dikhegi?\n",
      "    **Shagird (student):** Andhera!\n",
      "\n",
      "2.  **Doctor:** Kya hua? Tum itne pareshaan kyun ho?\n",
      "    **Mareez (patient):** Meri biwi ke daant mein dard hai.\n",
      "    **Doctor:** Toh mein kya karoon?\n",
      "    **Mareez:** Bas aap muskura deejiye, woh hamesha kehti hai ke aapke daant kitne khoobsurat hain!\n",
      "\n",
      "3.  **Ek aadmi apni biwi se:** \"Main tumhe cinema le jaata, lekin tum toh kehti ho wahan to tum sotee ho.\"\n",
      "    **Biwi:** \"Arre wah, phir toh zaroor chaloongi. Akele ghar mein kaun sone dega?\"\n",
      "\n",
      "4.  **Ek aadmi dukaan par jaata hai aur puchta hai:** \"Bhaiya, anday hain?\"\n",
      "    **Dukaan daar (shopkeeper):** \"Haan hain.\"\n",
      "    **Aadmi:** \"Achha, yeh bhi batana, murgi ke hain ya bakri ke?\"\n",
      "\n",
      "5.  **Pappu:** Yaar, mujhe ek aisi naukri ki talaash hai jismein zyada kaam na karna pade.\n",
      "    **Friend:** Toh tum police mein kyun nahi try karte?\n",
      "    **Pappu:** Kyun?\n",
      "    **Friend:** Kyunke, police ko toh har koi dhoondhta rehta hai, koi unhe dhoondhne nahi jaata!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "from agents import AgentOutputSchema \n",
    "from agents import Agent \n",
    " \n",
    "    \n",
    "old_agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        \n",
    "        model=model,\n",
    "        \n",
    "       \n",
    "    )\n",
    "\n",
    "result = Runner.run_sync(old_agent, input=\"Please tell me 5 jokes.\")\n",
    "print(result.final_output)\n",
    "print('*******************')\n",
    "new_agent = old_agent.clone(\n",
    "    name='new_agent',\n",
    "    instructions='you are an helpful assistant response in urdu',\n",
    "  \n",
    ")\n",
    "result = Runner.run_sync(new_agent, input=\"Please tell me 5 jokes.\")\n",
    "print(result.final_output)\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bcf7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import function_tool\n",
    "\n",
    "@function_tool\n",
    "def find_priminster(country:str):\n",
    "    resp =  f\"Priminister of {country} is Imran Khan\"\n",
    "    print(resp)\n",
    "    return resp\n",
    "@function_tool\n",
    "def find_weather(city:str)->str:\n",
    "    resp =  f\"The Weather of {city} is very Sunny\"\n",
    "    print(resp)\n",
    "    return resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "511e0dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot access weather information. I can only tell you who the priminster of pakistan is. Would you like me to do that?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "from agents import AgentOutputSchema \n",
    "from agents import Agent \n",
    " \n",
    "    \n",
    "old_agent = Agent(\n",
    "        name=\"old_agent\",\n",
    "        tools=[find_priminster],\n",
    "        instructions='you are helpful assistant ',\n",
    "        model=model,\n",
    "        tool_use_behavior='stop_on_first_tool'\n",
    "    )\n",
    "\n",
    "result = Runner.run_sync(old_agent, input=\"who is the priminster of pakistan and what is current weather \")\n",
    "print(result.final_output)\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d0ab871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************\n",
      "The Weather of sialkot is very Sunny\n",
      "Priminister of pakistan is Imran Khan\n",
      "The Weather of sialkot is very Sunny\n"
     ]
    }
   ],
   "source": [
    "print('*******************')\n",
    "new_agent = old_agent.clone(\n",
    "    name='new_agent',\n",
    "    instructions='you are an helpful assistant response in urdu',\n",
    "    tools=[find_weather,find_priminster]\n",
    ")\n",
    "result = Runner.run_sync(new_agent, input=\"what is the weather of sialkot and who is the priminster of pakistan \")\n",
    "print(result.final_output)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62610ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am sorry, I cannot provide weather information about Sialkot. I can only find the priminster of pakistan. Would you like me to do that?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = Runner.run_sync(old_agent, input=\"what is the weather of sialkot and who is the priminster of pakistan  \")\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b64032cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Agent Output: I'm sorry, I can only find the priminster of a country. I cannot access weather information. Would you like me to search for the priminster of pakistan?\n",
      "\n",
      "*******************\n",
      "New Agent tools after modification: [[FunctionTool(name='find_priminster', description='', params_json_schema={'properties': {'country': {'title': 'Country', 'type': 'string'}}, 'required': ['country'], 'title': 'find_priminster_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153054AE0>, strict_json_schema=True, is_enabled=True), FunctionTool(name='find_weather', description='', params_json_schema={'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], 'title': 'find_weather_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153054900>, strict_json_schema=True, is_enabled=True)]]\n",
      "Old Agent tools after modification: [[FunctionTool(name='find_priminster', description='', params_json_schema={'properties': {'country': {'title': 'Country', 'type': 'string'}}, 'required': ['country'], 'title': 'find_priminster_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153054AE0>, strict_json_schema=True, is_enabled=True), FunctionTool(name='find_weather', description='', params_json_schema={'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], 'title': 'find_weather_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153054900>, strict_json_schema=True, is_enabled=True)]]\n",
      "The Weather of sialkot is very Sunny\n",
      "Priminister of pakistan is Imran Khan\n",
      "New Agent Output: The Weather of sialkot is very Sunny\n",
      "The Weather of sialkot is very Sunny\n",
      "Priminister of pakistan is Imran Khan\n",
      "Old Agent Output (after modification): The Weather of sialkot is very Sunny\n"
     ]
    }
   ],
   "source": [
    "from agents import function_tool, Agent, Runner\n",
    "\n",
    "@function_tool\n",
    "def find_priminster(country: str):\n",
    "    resp = f\"Priminister of {country} is Imran Khan\"\n",
    "    print(resp)\n",
    "    return resp\n",
    "\n",
    "@function_tool\n",
    "def find_weather(city: str) -> str:\n",
    "    resp = f\"The Weather of {city} is very Sunny\"\n",
    "    print(resp)\n",
    "    return resp\n",
    "\n",
    "# Create original agent\n",
    "old_agent = Agent(\n",
    "    name=\"old_agent\",\n",
    "    tools=[find_priminster],\n",
    "    instructions='you are helpful assistant',\n",
    "    model=model,  # Assume model is valid\n",
    "    tool_use_behavior='stop_on_first_tool'\n",
    ")\n",
    "\n",
    "# Run original agent\n",
    "result = Runner.run_sync(old_agent, input=\"who is the priminster of pakistan and what is current weather \")\n",
    "print(\"Old Agent Output:\", result.final_output)\n",
    "# Output: Priminister of pakistan is Imran Khan\n",
    "\n",
    "print('*******************')\n",
    "\n",
    "# Clone without overriding tools\n",
    "new_agent = old_agent.clone(\n",
    "    name='new_agent',\n",
    "    instructions='you are an helpful assistant response in urdu'\n",
    ")  # Note: tools not specified\n",
    "\n",
    "# Modify new_agent.tools\n",
    "new_agent.tools.append(find_weather)\n",
    "print(f\"New Agent tools after modification: {[ new_agent.tools]}\")\n",
    "print(f\"Old Agent tools after modification: {[  old_agent.tools]}\")\n",
    "\n",
    "# Run both agents\n",
    "result_new = Runner.run_sync(new_agent, input=\"what is the weather of sialkot and who is the priminster of pakistan \")\n",
    "print(\"New Agent Output:\", result_new.final_output)\n",
    "\n",
    "result_old = Runner.run_sync(old_agent, input=\"what is the weather of sialkot and who is the priminster of pakistan \")\n",
    "print(\"Old Agent Output (after modification):\", result_old.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96e606de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FunctionTool(name='tool1', description='', params_json_schema={'properties': {}, 'title': 'tool1_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x00000251530680E0>, strict_json_schema=True, is_enabled=True)]\n",
      "[FunctionTool(name='tool1', description='', params_json_schema={'properties': {}, 'title': 'tool1_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x00000251530680E0>, strict_json_schema=True, is_enabled=True), FunctionTool(name='tool2', description='', params_json_schema={'properties': {}, 'title': 'tool2_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153068400>, strict_json_schema=True, is_enabled=True)]\n",
      "[FunctionTool(name='tool1', description='', params_json_schema={'properties': {}, 'title': 'tool1_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x00000251530680E0>, strict_json_schema=True, is_enabled=True), FunctionTool(name='tool2', description='', params_json_schema={'properties': {}, 'title': 'tool2_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153068400>, strict_json_schema=True, is_enabled=True)]\n"
     ]
    }
   ],
   "source": [
    "from agents import function_tool, Agent\n",
    "\n",
    "@function_tool\n",
    "def tool1(): return \"Tool 1\"\n",
    "@function_tool\n",
    "def tool2(): return \"Tool 2\"\n",
    "\n",
    "old_agent = Agent(name=\"old\", tools=[tool1],model=model)\n",
    "\n",
    "print(old_agent.tools)  # [tool1, tool2]\n",
    "new_agent = old_agent.clone(name=\"new\")  # No tools override\n",
    "new_agent.tools.append(tool2)  # Modify cloned agent's tools\n",
    "print(old_agent.tools)  # [tool1, tool2]\n",
    "print(new_agent.tools)  # [tool1, tool2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "133b0526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FunctionTool(name='tool1', description='', params_json_schema={'properties': {}, 'title': 'tool1_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153068B80>, strict_json_schema=True, is_enabled=True)]\n",
      "[FunctionTool(name='tool1', description='', params_json_schema={'properties': {}, 'title': 'tool1_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153068B80>, strict_json_schema=True, is_enabled=True), FunctionTool(name='tool2', description='', params_json_schema={'properties': {}, 'title': 'tool2_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x00000251530696C0>, strict_json_schema=True, is_enabled=True)]\n",
      "[FunctionTool(name='tool1', description='', params_json_schema={'properties': {}, 'title': 'tool1_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153068B80>, strict_json_schema=True, is_enabled=True), FunctionTool(name='tool2', description='', params_json_schema={'properties': {}, 'title': 'tool2_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x00000251530696C0>, strict_json_schema=True, is_enabled=True)]\n"
     ]
    }
   ],
   "source": [
    "from agents import function_tool, Agent\n",
    "\n",
    "@function_tool\n",
    "def tool1(): return \"Tool 1\"\n",
    "@function_tool\n",
    "def tool2(): return \"Tool 2\"\n",
    "\n",
    "old_agent = Agent(name=\"old\", tools=[tool1],model=model)\n",
    "\n",
    "print(old_agent.tools)  # [tool1, tool2]\n",
    "new_agent = old_agent.clone(name=\"new\")  # No tools override\n",
    "old_agent.tools.append(tool2)  # Modify cloned agent's tools\n",
    "print(old_agent.tools)  # [tool1, tool2]\n",
    "print(new_agent.tools)  # [tool1, tool2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48c42490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FunctionTool(name='tool1', description='', params_json_schema={'properties': {}, 'title': 'tool1_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153069620>, strict_json_schema=True, is_enabled=True)]\n",
      "[FunctionTool(name='tool1', description='', params_json_schema={'properties': {}, 'title': 'tool1_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153069620>, strict_json_schema=True, is_enabled=True)]\n",
      "[FunctionTool(name='tool1', description='', params_json_schema={'properties': {}, 'title': 'tool1_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153069620>, strict_json_schema=True, is_enabled=True), FunctionTool(name='tool2', description='', params_json_schema={'properties': {}, 'title': 'tool2_args', 'type': 'object', 'additionalProperties': False, 'required': []}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x0000025153069BC0>, strict_json_schema=True, is_enabled=True)]\n"
     ]
    }
   ],
   "source": [
    "from agents import function_tool, Agent\n",
    "\n",
    "@function_tool\n",
    "def tool1(): return \"Tool 1\"\n",
    "@function_tool\n",
    "def tool2(): return \"Tool 2\"\n",
    "\n",
    "old_agent = Agent(name=\"old\", tools=[tool1],model=model)\n",
    "\n",
    "print(old_agent.tools)  # [tool1, tool2]\n",
    "new_agent = old_agent.clone(name=\"new\",tools=[tool1])  # No tools override\n",
    "new_agent.tools.append(tool2)  # Modify cloned agent's tools\n",
    "print(old_agent.tools)  # [tool1, tool2]\n",
    "print(new_agent.tools)  # [tool1, tool2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8af5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM input messages: \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "An asyncio.Future, a coroutine or an awaitable is required",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.final_output)  \u001b[38;5;66;03m# JokeOutput instance\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:306\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    304\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    309\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Run the agent\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(agent, \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mTell me a joke.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.final_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:206\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[33;03moutput is generated. The loop runs like so:\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m1. The agent is invoked with the given input.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    203\u001b[39m \u001b[33;03m    agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    205\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    207\u001b[39m     starting_agent,\n\u001b[32m    208\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    209\u001b[39m     context=context,\n\u001b[32m    210\u001b[39m     max_turns=max_turns,\n\u001b[32m    211\u001b[39m     hooks=hooks,\n\u001b[32m    212\u001b[39m     run_config=run_config,\n\u001b[32m    213\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    214\u001b[39m     session=session,\n\u001b[32m    215\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:412\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m logger.debug(\n\u001b[32m    408\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    409\u001b[39m )\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    413\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    414\u001b[39m             starting_agent,\n\u001b[32m    415\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    416\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    417\u001b[39m             copy.deepcopy(prepared_input),\n\u001b[32m    418\u001b[39m             context_wrapper,\n\u001b[32m    419\u001b[39m         ),\n\u001b[32m    420\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    421\u001b[39m             agent=current_agent,\n\u001b[32m    422\u001b[39m             all_tools=all_tools,\n\u001b[32m    423\u001b[39m             original_input=original_input,\n\u001b[32m    424\u001b[39m             generated_items=generated_items,\n\u001b[32m    425\u001b[39m             hooks=hooks,\n\u001b[32m    426\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    427\u001b[39m             run_config=run_config,\n\u001b[32m    428\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    429\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    430\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    431\u001b[39m         ),\n\u001b[32m    432\u001b[39m     )\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    435\u001b[39m         agent=current_agent,\n\u001b[32m    436\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    445\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:941\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_single_turn\u001b[39m(\n\u001b[32m    926\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m ) -> SingleStepResult:\n\u001b[32m    939\u001b[39m     \u001b[38;5;66;03m# Ensure we run the hooks before anything else\u001b[39;00m\n\u001b[32m    940\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_run_agent_start_hooks:\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_agent_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_coro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnoop_coroutine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    950\u001b[39m     system_prompt, prompt_config = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    951\u001b[39m         agent.get_system_prompt(context_wrapper),\n\u001b[32m    952\u001b[39m         agent.get_prompt(context_wrapper),\n\u001b[32m    953\u001b[39m     )\n\u001b[32m    955\u001b[39m     output_schema = \u001b[38;5;28mcls\u001b[39m._get_output_schema(agent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:884\u001b[39m, in \u001b[36mgather\u001b[39m\u001b[34m(return_exceptions, *coros_or_futures)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m coros_or_futures:\n\u001b[32m    883\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m arg_to_fut:\n\u001b[32m--> \u001b[39m\u001b[32m884\u001b[39m         fut = \u001b[43mensure_future\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    885\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    886\u001b[39m             loop = futures._get_loop(fut)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:742\u001b[39m, in \u001b[36mensure_future\u001b[39m\u001b[34m(coro_or_future, loop)\u001b[39m\n\u001b[32m    740\u001b[39m         should_close = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mAn asyncio.Future, a coroutine or an awaitable \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    743\u001b[39m                         \u001b[33m'\u001b[39m\u001b[33mis required\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    745\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    746\u001b[39m     loop = events.get_event_loop()\n",
      "\u001b[31mTypeError\u001b[39m: An asyncio.Future, a coroutine or an awaitable is required"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional,Any\n",
    "from agents import Agent, AgentHooks, AgentOutputSchema, Runner,RunContextWrapper\n",
    "from pydantic import BaseModel\n",
    "# User-defined class (Pydantic or dataclass)\n",
    "# @dataclass\n",
    "class JokeOutput(BaseModel):\n",
    "    joke: str\n",
    "    rating: Optional[int] = None\n",
    "\n",
    "# Custom hooks for logging\n",
    "class JokeHooks(AgentHooks):\n",
    "    def on_start(self, run_context: RunContextWrapper,agent: Agent) -> None:\n",
    "        print(f\"LLM input messages: \")\n",
    "\n",
    "    # def on_end(self, run_context: RunContextWrapper, output: Any) -> None:\n",
    "    #     print(f\"Final output (converted): {output}\")\n",
    "\n",
    "# Create agent with user-defined output type and hooks\n",
    "agent = Agent(\n",
    "    name=\"JokeAgent\",\n",
    "    instructions=\"Generate a joke with an optional rating\",\n",
    "    # output_type=JokeOutput,\n",
    "    hooks=JokeHooks(),\n",
    "    model=model\n",
    ")\n",
    "async def main():\n",
    "    # Run the agent\n",
    "    result = await Runner.run(agent, input=\"Tell me a joke.\")\n",
    "    print(result.final_output)  # JokeOutput instance\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa08c50c",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - [{'error': {'code': 400, 'message': \"Unable to submit request because one or more response schemas didn't specify the schema type field. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output\", 'status': 'INVALID_ARGUMENT'}}]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.final_output)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     result = \u001b[43mRunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPlease tell me 5 jokes.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.final_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:260\u001b[39m, in \u001b[36mRunner.run_sync\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a workflow synchronously, starting at the given agent. Note that this just wraps the\u001b[39;00m\n\u001b[32m    231\u001b[39m \u001b[33;03m`run` method, so it will not work if there's already an event loop (e.g. inside an async\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[33;03mfunction, or in a Jupyter notebook or async context like FastAPI). For those cases, use\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m \u001b[33;03m    agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstarting_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:513\u001b[39m, in \u001b[36mAgentRunner.run_sync\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    510\u001b[39m previous_response_id = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    511\u001b[39m session = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33msession\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstarting_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:306\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    304\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    309\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:412\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m logger.debug(\n\u001b[32m    408\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    409\u001b[39m )\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    413\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    414\u001b[39m             starting_agent,\n\u001b[32m    415\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    416\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    417\u001b[39m             copy.deepcopy(prepared_input),\n\u001b[32m    418\u001b[39m             context_wrapper,\n\u001b[32m    419\u001b[39m         ),\n\u001b[32m    420\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    421\u001b[39m             agent=current_agent,\n\u001b[32m    422\u001b[39m             all_tools=all_tools,\n\u001b[32m    423\u001b[39m             original_input=original_input,\n\u001b[32m    424\u001b[39m             generated_items=generated_items,\n\u001b[32m    425\u001b[39m             hooks=hooks,\n\u001b[32m    426\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    427\u001b[39m             run_config=run_config,\n\u001b[32m    428\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    429\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    430\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    431\u001b[39m         ),\n\u001b[32m    432\u001b[39m     )\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    435\u001b[39m         agent=current_agent,\n\u001b[32m    436\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    445\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:960\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    958\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    961\u001b[39m     agent,\n\u001b[32m    962\u001b[39m     system_prompt,\n\u001b[32m    963\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    964\u001b[39m     output_schema,\n\u001b[32m    965\u001b[39m     all_tools,\n\u001b[32m    966\u001b[39m     handoffs,\n\u001b[32m    967\u001b[39m     context_wrapper,\n\u001b[32m    968\u001b[39m     run_config,\n\u001b[32m    969\u001b[39m     tool_use_tracker,\n\u001b[32m    970\u001b[39m     previous_response_id,\n\u001b[32m    971\u001b[39m     prompt_config,\n\u001b[32m    972\u001b[39m )\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    975\u001b[39m     agent=agent,\n\u001b[32m    976\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    985\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    986\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:1121\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id, prompt_config)\u001b[39m\n\u001b[32m   1118\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m   1119\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1122\u001b[39m     system_instructions=system_prompt,\n\u001b[32m   1123\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1124\u001b[39m     model_settings=model_settings,\n\u001b[32m   1125\u001b[39m     tools=all_tools,\n\u001b[32m   1126\u001b[39m     output_schema=output_schema,\n\u001b[32m   1127\u001b[39m     handoffs=handoffs,\n\u001b[32m   1128\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1129\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1130\u001b[39m     ),\n\u001b[32m   1131\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1132\u001b[39m     prompt=prompt_config,\n\u001b[32m   1133\u001b[39m )\n\u001b[32m   1135\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:65\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, prompt)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     50\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     prompt: ResponsePromptParam | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     59\u001b[39m ) -> ModelResponse:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m     61\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m     62\u001b[39m         model_config=model_settings.to_json_dict() | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._client.base_url)},\n\u001b[32m     63\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m     64\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     66\u001b[39m             system_instructions,\n\u001b[32m     67\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     68\u001b[39m             model_settings,\n\u001b[32m     69\u001b[39m             tools,\n\u001b[32m     70\u001b[39m             output_schema,\n\u001b[32m     71\u001b[39m             handoffs,\n\u001b[32m     72\u001b[39m             span_generation,\n\u001b[32m     73\u001b[39m             tracing,\n\u001b[32m     74\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     75\u001b[39m             prompt=prompt,\n\u001b[32m     76\u001b[39m         )\n\u001b[32m     78\u001b[39m         message: ChatCompletionMessage | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     79\u001b[39m         first_choice: Choice | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:273\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream, prompt)\u001b[39m\n\u001b[32m    267\u001b[39m store = ChatCmplHelpers.get_store_param(\u001b[38;5;28mself\u001b[39m._get_client(), model_settings)\n\u001b[32m    269\u001b[39m stream_options = ChatCmplHelpers.get_stream_options_param(\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_client(), model_settings, stream=stream\n\u001b[32m    271\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_client().chat.completions.create(\n\u001b[32m    274\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    275\u001b[39m     messages=converted_messages,\n\u001b[32m    276\u001b[39m     tools=converted_tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    277\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    278\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    279\u001b[39m     frequency_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.frequency_penalty),\n\u001b[32m    280\u001b[39m     presence_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.presence_penalty),\n\u001b[32m    281\u001b[39m     max_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    282\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    283\u001b[39m     response_format=response_format,\n\u001b[32m    284\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    285\u001b[39m     stream=stream,\n\u001b[32m    286\u001b[39m     stream_options=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(stream_options),\n\u001b[32m    287\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(store),\n\u001b[32m    288\u001b[39m     reasoning_effort=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(reasoning_effort),\n\u001b[32m    289\u001b[39m     extra_headers={**HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    290\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    291\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    292\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    293\u001b[39m     **(model_settings.extra_args \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    294\u001b[39m )\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, ChatCompletion):\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2454\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2411\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2412\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2413\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2451\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2452\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2453\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2455\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2456\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2457\u001b[39m             {\n\u001b[32m   2458\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2460\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2461\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2462\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2463\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2464\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2465\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2466\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2467\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2468\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2469\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2470\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2471\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2472\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2473\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2474\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2475\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2476\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2477\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2478\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2479\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2480\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2481\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2482\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2483\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2484\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2485\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2486\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2487\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2488\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2489\u001b[39m             },\n\u001b[32m   2490\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2491\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2492\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2493\u001b[39m         ),\n\u001b[32m   2494\u001b[39m         options=make_request_options(\n\u001b[32m   2495\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2496\u001b[39m         ),\n\u001b[32m   2497\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2498\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2499\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2500\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1791\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1779\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1786\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1787\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1788\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1789\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1790\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1591\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1588\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1590\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1591\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1593\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - [{'error': {'code': 400, 'message': \"Unable to submit request because one or more response schemas didn't specify the schema type field. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output\", 'status': 'INVALID_ARGUMENT'}}]"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Any\n",
    "from agents import Agent, AgentHooks, Runner, RunContextWrapper\n",
    "from pydantic import BaseModel\n",
    "import asyncio\n",
    "\n",
    "import asyncio \n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "from agents import Agent\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# User-defined class (Pydantic)\n",
    "class OutputTypes(BaseModel):\n",
    "    joke: list[str]\n",
    "    language:Optional[str]=None\n",
    "    # rating: Optional[int] = None\n",
    "\n",
    "# Custom hooks for logging\n",
    "class JokeHooks(AgentHooks):\n",
    "    def on_start(self, run_context: RunContextWrapper, agent: Agent) -> None:\n",
    "        print(f\"LLM input messages: \")\n",
    " \n",
    "old_agent = Agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        model=model,\n",
    "        output_type=OutputTypes\n",
    "       \n",
    "    )\n",
    "async def main():\n",
    "    \n",
    "\n",
    "    result = Runner.run_sync(old_agent, input=\"Please tell me 5 jokes.\")\n",
    "    print(result.final_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5ce42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
