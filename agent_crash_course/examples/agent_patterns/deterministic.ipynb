{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7fe748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply() \n",
    "import os\n",
    "from agents import Agent, OpenAIChatCompletionsModel, AsyncOpenAI, Runner\n",
    "import asyncio\n",
    "try:\n",
    "\n",
    "    GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not GEMINI_API_KEY:\n",
    "        raise ValueError(\"GEMINI ApI key not found\")\n",
    "  \n",
    "    external_client = AsyncOpenAI(\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "        api_key=GEMINI_API_KEY,\n",
    "    )\n",
    "    model = OpenAIChatCompletionsModel(\n",
    "        model=\"gemini-2.0-flash\", openai_client=external_client\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9882ba50",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect(request)\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:124\u001b[39m, in \u001b[36mAsyncHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_backend.connect_tcp(**kwargs)\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpcore\\_backends\\auto.py:31\u001b[39m, in \u001b[36mAutoBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_backend()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.connect_tcp(\n\u001b[32m     32\u001b[39m     host,\n\u001b[32m     33\u001b[39m     port,\n\u001b[32m     34\u001b[39m     timeout=timeout,\n\u001b[32m     35\u001b[39m     local_address=local_address,\n\u001b[32m     36\u001b[39m     socket_options=socket_options,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:113\u001b[39m, in \u001b[36mAnyIOBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    108\u001b[39m exc_map = {\n\u001b[32m    109\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ConnectTimeout,\n\u001b[32m    110\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    111\u001b[39m     anyio.BrokenResourceError: ConnectError,\n\u001b[32m    112\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1526\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1525\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1526\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1527\u001b[39m         request,\n\u001b[32m   1528\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1529\u001b[39m         **kwargs,\n\u001b[32m   1530\u001b[39m     )\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpx\\_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpx\\_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpx\\_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpx\\_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py:162\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: All connection attempts failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.final_output)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHi what is weather in sialkot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m   \n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:306\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    304\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    309\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(input)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m(\u001b[38;5;28minput\u001b[39m: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(triage_agent, \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.final_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:206\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[33;03moutput is generated. The loop runs like so:\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m1. The agent is invoked with the given input.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    203\u001b[39m \u001b[33;03m    agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    205\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    207\u001b[39m     starting_agent,\n\u001b[32m    208\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    209\u001b[39m     context=context,\n\u001b[32m    210\u001b[39m     max_turns=max_turns,\n\u001b[32m    211\u001b[39m     hooks=hooks,\n\u001b[32m    212\u001b[39m     run_config=run_config,\n\u001b[32m    213\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    214\u001b[39m     session=session,\n\u001b[32m    215\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:412\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m logger.debug(\n\u001b[32m    408\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    409\u001b[39m )\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    413\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    414\u001b[39m             starting_agent,\n\u001b[32m    415\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    416\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    417\u001b[39m             copy.deepcopy(prepared_input),\n\u001b[32m    418\u001b[39m             context_wrapper,\n\u001b[32m    419\u001b[39m         ),\n\u001b[32m    420\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    421\u001b[39m             agent=current_agent,\n\u001b[32m    422\u001b[39m             all_tools=all_tools,\n\u001b[32m    423\u001b[39m             original_input=original_input,\n\u001b[32m    424\u001b[39m             generated_items=generated_items,\n\u001b[32m    425\u001b[39m             hooks=hooks,\n\u001b[32m    426\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    427\u001b[39m             run_config=run_config,\n\u001b[32m    428\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    429\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    430\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    431\u001b[39m         ),\n\u001b[32m    432\u001b[39m     )\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    435\u001b[39m         agent=current_agent,\n\u001b[32m    436\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    445\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:960\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    958\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    961\u001b[39m     agent,\n\u001b[32m    962\u001b[39m     system_prompt,\n\u001b[32m    963\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    964\u001b[39m     output_schema,\n\u001b[32m    965\u001b[39m     all_tools,\n\u001b[32m    966\u001b[39m     handoffs,\n\u001b[32m    967\u001b[39m     context_wrapper,\n\u001b[32m    968\u001b[39m     run_config,\n\u001b[32m    969\u001b[39m     tool_use_tracker,\n\u001b[32m    970\u001b[39m     previous_response_id,\n\u001b[32m    971\u001b[39m     prompt_config,\n\u001b[32m    972\u001b[39m )\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    975\u001b[39m     agent=agent,\n\u001b[32m    976\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    985\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    986\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:1121\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id, prompt_config)\u001b[39m\n\u001b[32m   1118\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m   1119\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1122\u001b[39m     system_instructions=system_prompt,\n\u001b[32m   1123\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1124\u001b[39m     model_settings=model_settings,\n\u001b[32m   1125\u001b[39m     tools=all_tools,\n\u001b[32m   1126\u001b[39m     output_schema=output_schema,\n\u001b[32m   1127\u001b[39m     handoffs=handoffs,\n\u001b[32m   1128\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1129\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1130\u001b[39m     ),\n\u001b[32m   1131\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1132\u001b[39m     prompt=prompt_config,\n\u001b[32m   1133\u001b[39m )\n\u001b[32m   1135\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:65\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, prompt)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     50\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     prompt: ResponsePromptParam | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     59\u001b[39m ) -> ModelResponse:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m     61\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m     62\u001b[39m         model_config=model_settings.to_json_dict() | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._client.base_url)},\n\u001b[32m     63\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m     64\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     66\u001b[39m             system_instructions,\n\u001b[32m     67\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     68\u001b[39m             model_settings,\n\u001b[32m     69\u001b[39m             tools,\n\u001b[32m     70\u001b[39m             output_schema,\n\u001b[32m     71\u001b[39m             handoffs,\n\u001b[32m     72\u001b[39m             span_generation,\n\u001b[32m     73\u001b[39m             tracing,\n\u001b[32m     74\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     75\u001b[39m             prompt=prompt,\n\u001b[32m     76\u001b[39m         )\n\u001b[32m     78\u001b[39m         message: ChatCompletionMessage | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     79\u001b[39m         first_choice: Choice | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:273\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream, prompt)\u001b[39m\n\u001b[32m    267\u001b[39m store = ChatCmplHelpers.get_store_param(\u001b[38;5;28mself\u001b[39m._get_client(), model_settings)\n\u001b[32m    269\u001b[39m stream_options = ChatCmplHelpers.get_stream_options_param(\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_client(), model_settings, stream=stream\n\u001b[32m    271\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_client().chat.completions.create(\n\u001b[32m    274\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    275\u001b[39m     messages=converted_messages,\n\u001b[32m    276\u001b[39m     tools=converted_tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    277\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    278\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    279\u001b[39m     frequency_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.frequency_penalty),\n\u001b[32m    280\u001b[39m     presence_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.presence_penalty),\n\u001b[32m    281\u001b[39m     max_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    282\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    283\u001b[39m     response_format=response_format,\n\u001b[32m    284\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    285\u001b[39m     stream=stream,\n\u001b[32m    286\u001b[39m     stream_options=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(stream_options),\n\u001b[32m    287\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(store),\n\u001b[32m    288\u001b[39m     reasoning_effort=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(reasoning_effort),\n\u001b[32m    289\u001b[39m     extra_headers={**HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    290\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    291\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    292\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    293\u001b[39m     **(model_settings.extra_args \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    294\u001b[39m )\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, ChatCompletion):\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2454\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2411\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2412\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2413\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2451\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2452\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2453\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2455\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2456\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2457\u001b[39m             {\n\u001b[32m   2458\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2460\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2461\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2462\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2463\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2464\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2465\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2466\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2467\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2468\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2469\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2470\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2471\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2472\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2473\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2474\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2475\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2476\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2477\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2478\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2479\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2480\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2481\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2482\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2483\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2484\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2485\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2486\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2487\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2488\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2489\u001b[39m             },\n\u001b[32m   2490\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2491\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2492\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2493\u001b[39m         ),\n\u001b[32m   2494\u001b[39m         options=make_request_options(\n\u001b[32m   2495\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2496\u001b[39m         ),\n\u001b[32m   2497\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2498\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2499\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2500\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1791\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1779\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1786\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1787\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1788\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1789\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1790\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1558\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1555\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1557\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1558\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1560\u001b[39m log.debug(\n\u001b[32m   1561\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1562\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1566\u001b[39m     response.headers,\n\u001b[32m   1567\u001b[39m )\n\u001b[32m   1568\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error."
     ]
    }
   ],
   "source": [
    "from agents import Agent, HandoffInputFilter, function_tool, handoff, RunContextWrapper\n",
    "from agents.extensions import handoff_filters\n",
    "urdu_agent = Agent(\n",
    "    name=\"Urdu agent\",\n",
    "    model=model,\n",
    "    instructions=\"You only speak Urdu.\"\n",
    ")\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city:str):\n",
    "    resp=f'Weather of {city} is charmming'\n",
    "    print(f\"resp:{resp}\")\n",
    "    return resp\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\",model=model,\n",
    "    instructions=\"You only speak English\",\n",
    "    tools=[get_weather]\n",
    ")\n",
    " \n",
    "triage_agent = Agent(\n",
    "    name=\"Triage agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
    "    handoffs=[\n",
    "            handoff(urdu_agent ),\n",
    "            handoff(english_agent,  input_filter=handoff_filters.remove_all_tools,)\n",
    "    ],model = model\n",
    ")\n",
    "\n",
    "\n",
    "async def main(input: str):\n",
    "    result = await Runner.run(triage_agent, input=input,)\n",
    "    print(result.final_output)\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main(\"Hi what is weather in sialkot\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, HandoffInputFilter, function_tool, handoff, RunContextWrapper\n",
    "from agents.extensions import handoff_filters\n",
    "urdu_agent = Agent(\n",
    "    name=\"Urdu agent\",\n",
    "    model=model,\n",
    "    instructions=\"You only speak Urdu.\"\n",
    ")\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city:str):\n",
    "    resp=f'Weather of {city} is charmming'\n",
    "    print(f\"resp:{resp}\")\n",
    "    return resp\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\",model=model,\n",
    "    instructions=\"You only speak English\",\n",
    "    tools=[get_weather]\n",
    ")\n",
    " \n",
    "triage_agent = Agent(\n",
    "    name=\"Triage agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
    "    handoffs=[\n",
    "            handoff(urdu_agent ),\n",
    "            handoff(english_agent,  input_filter=handoff_filters.remove_all_tools,)\n",
    "    ],model = model\n",
    ")\n",
    "\n",
    "\n",
    "async def main(input: str):\n",
    "    result = await Runner.run(triage_agent, input=input,)\n",
    "    print(result.final_output)\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main(\"Hi what is weather in sialkot\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0adeec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Handing off to Urdu agent...\n",
      "--------------------------------\n",
      "وعلیکم السلام! کیا حال ہے؟ میں آپ کی کیا مدد کر سکتا ہوں؟\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, handoff, RunContextWrapper\n",
    "\n",
    "urdu_agent = Agent(\n",
    "    name=\"Urdu agent\",\n",
    "    model=model,\n",
    "    instructions=\"You only speak Urdu.\"\n",
    ")\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\",model=model,\n",
    "    instructions=\"You only speak English\"\n",
    ")\n",
    "\n",
    "def on_handoff(agent: Agent, ctx: RunContextWrapper[None]):\n",
    "    agent_name = agent.name\n",
    "    print(\"--------------------------------\")\n",
    "    print(f\"Handing off to {agent_name}...\")\n",
    "    print(\"--------------------------------\")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
    "    handoffs=[\n",
    "            handoff(urdu_agent, on_handoff=lambda ctx: on_handoff(urdu_agent, ctx)),\n",
    "            handoff(english_agent, on_handoff=lambda ctx: on_handoff(english_agent, ctx))\n",
    "    ],model = model\n",
    ")\n",
    "\n",
    "\n",
    "async def main(input: str):\n",
    "    result = await Runner.run(triage_agent, input=input,)\n",
    "    print(result.final_output)\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main(\"السلام عليكم\"))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8955935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outline generated\n",
      "Outline is not a scifi story, so we stop here.\n",
      "Outline is good quality and a scifi story, so we continue to write the story.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Event loop stopped before Future completed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:96\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m f.result()\n",
      "\u001b[31mRuntimeError\u001b[39m: Event loop stopped before Future completed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     77\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstory_result.final_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:35\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     33\u001b[39m task.cancel()\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m suppress(asyncio.CancelledError):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:96\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m f.result()\n",
      "\u001b[31mRuntimeError\u001b[39m: Event loop stopped before Future completed."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is not set, skipping trace export\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from agents import Agent, Runner, trace\n",
    "\n",
    "\"\"\"\n",
    "This example demonstrates a deterministic flow, where each step is performed by an agent.\n",
    "1. The first agent generates a story outline\n",
    "2. We feed the outline into the second agent\n",
    "3. The second agent checks if the outline is good quality and if it is a scifi story\n",
    "4. If the outline is not good quality or not a scifi story, we stop here\n",
    "5. If the outline is good quality and a scifi story, we feed the outline into the third agent\n",
    "6. The third agent writes the story\n",
    "\"\"\"\n",
    "\n",
    "story_outline_agent = Agent(\n",
    "    name=\"story_outline_agent\",\n",
    "    instructions=\"Generate a very short story outline based on the user's input.\",model=model\n",
    ")\n",
    "\n",
    "\n",
    "class OutlineCheckerOutput(BaseModel):\n",
    "    good_quality: bool\n",
    "    is_scifi: bool\n",
    "\n",
    "\n",
    "outline_checker_agent = Agent(\n",
    "    name=\"outline_checker_agent\",\n",
    "    instructions=\"Read the given story outline, and judge the quality. Also, determine if it is a scifi story.\",\n",
    "    output_type=OutlineCheckerOutput,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "story_agent = Agent(\n",
    "    name=\"story_agent\",\n",
    "    instructions=\"Write a short story based on the given outline.\",\n",
    "    output_type=str,model=model\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    input_prompt = input(\"What kind of story do you want? \")\n",
    "\n",
    "    # Ensure the entire workflow is a single trace\n",
    "    with trace(\"Deterministic story flow\"):\n",
    "        # 1. Generate an outline\n",
    "        outline_result = await Runner.run(\n",
    "            story_outline_agent,\n",
    "            input_prompt,max_turns=2\n",
    "        )\n",
    "        print(\"Outline generated\")\n",
    "\n",
    "        # 2. Check the outline\n",
    "        outline_checker_result = await Runner.run(\n",
    "            outline_checker_agent,\n",
    "            outline_result.final_output,max_turns=2\n",
    "        )\n",
    "\n",
    "        # 3. Add a gate to stop if the outline is not good quality or not a scifi story\n",
    "        assert isinstance(outline_checker_result.final_output, OutlineCheckerOutput)\n",
    "        if not outline_checker_result.final_output.good_quality:\n",
    "            print(\"Outline is not good quality, so we stop here.\")\n",
    "            exit(0)\n",
    "\n",
    "        if not outline_checker_result.final_output.is_scifi:\n",
    "            print(\"Outline is not a scifi story, so we stop here.\")\n",
    "            exit(0)\n",
    "\n",
    "        print(\"Outline is good quality and a scifi story, so we continue to write the story.\")\n",
    "\n",
    "        # 4. Write the story\n",
    "        story_result = await Runner.run(\n",
    "            story_agent,\n",
    "            outline_result.final_output,max_turns=2\n",
    "        )\n",
    "        print(f\"Story: {story_result.final_output}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724fee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First message passed\n",
      "Guardrail didn't trip - this is unexpected. Output: {\n",
      "  \"reasoning\": \"I should not reveal the location based on the phone number. This would violate the user's privacy.\",\n",
      "  \"response\": \"I cannot reveal location information based on a phone number.\",\n",
      "  \"user_name\": \"unknown\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from agents import (\n",
    "    Agent,\n",
    "    GuardrailFunctionOutput,\n",
    "    OutputGuardrailTripwireTriggered,\n",
    "    RunContextWrapper,\n",
    "    Runner,\n",
    "    output_guardrail,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "This example shows how to use output guardrails.\n",
    "\n",
    "Output guardrails are checks that run on the final output of an agent.\n",
    "They can be used to do things like:\n",
    "- Check if the output contains sensitive data\n",
    "- Check if the output is a valid response to the user's message\n",
    "\n",
    "In this example, we'll use a (contrived) example where we check if the agent's response contains\n",
    "a phone number.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# The agent's output type\n",
    "class MessageOutput(BaseModel):\n",
    "    reasoning: str = Field(description=\"Thoughts on how to respond to the user's message\")\n",
    "    response: str = Field(description=\"The response to the user's message\")\n",
    "    user_name: str  = Field(description=\"The name of the user who sent the message, if known\")\n",
    "\n",
    "\n",
    "@output_guardrail\n",
    "async def sensitive_data_check(\n",
    "    context: RunContextWrapper, agent: Agent, output: MessageOutput\n",
    ") -> GuardrailFunctionOutput:\n",
    "    phone_number_in_response = \"650\" in output.response\n",
    "    phone_number_in_reasoning = \"650\" in output.reasoning\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info={\n",
    "            \"phone_number_in_response\": phone_number_in_response,\n",
    "            \"phone_number_in_reasoning\": phone_number_in_reasoning,\n",
    "        },\n",
    "        tripwire_triggered=phone_number_in_response or phone_number_in_reasoning,\n",
    "    )\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    output_type=MessageOutput,\n",
    "    output_guardrails=[sensitive_data_check],model=model\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # This should be ok\n",
    "    await Runner.run(agent, \"What's the capital of California?\")\n",
    "    print(\"First message passed\")\n",
    "\n",
    "    # This should trip the guardrail\n",
    "    try:\n",
    "        result = await Runner.run(\n",
    "            agent, \"My phone number is 650-123-4567. Where do you think I live?\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Guardrail didn't trip - this is unexpected. Output: {json.dumps(result.final_output.model_dump(), indent=2)}\"\n",
    "        )\n",
    "\n",
    "    except OutputGuardrailTripwireTriggered as e:\n",
    "        print(f\"Guardrail tripped. Info: {e.guardrail_result.output.output_info}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d663be04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error (expected): Strict JSON schema is enabled, but the output type is not valid. Either make the output type strict, or pass output_schema_strict=False to your Agent()\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - [{'error': {'code': 400, 'message': '* GenerateContentRequest.generation_config.response_schema.properties[\"response\"].properties[\"jokes\"].properties: should be non-empty for OBJECT type\\n', 'status': 'INVALID_ARGUMENT'}}]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result.final_output)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:306\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    304\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    309\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Now let's try again with a non-strict output type. This should work.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# In some cases, it will raise an error - the schema isn't strict, so the model may\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# produce an invalid JSON object.\u001b[39;00m\n\u001b[32m     57\u001b[39m agent.output_type = AgentOutputSchema(OutputType, strict_json_schema=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(agent, \u001b[38;5;28minput\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(result.final_output)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Finally, let's try a custom output type.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:206\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, session)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[33;03moutput is generated. The loop runs like so:\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[33;03m1. The agent is invoked with the given input.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    203\u001b[39m \u001b[33;03m    agent. Agents may perform handoffs, so we don't know the specific type of the output.\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    205\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    207\u001b[39m     starting_agent,\n\u001b[32m    208\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    209\u001b[39m     context=context,\n\u001b[32m    210\u001b[39m     max_turns=max_turns,\n\u001b[32m    211\u001b[39m     hooks=hooks,\n\u001b[32m    212\u001b[39m     run_config=run_config,\n\u001b[32m    213\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    214\u001b[39m     session=session,\n\u001b[32m    215\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:412\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    407\u001b[39m logger.debug(\n\u001b[32m    408\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    409\u001b[39m )\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    413\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    414\u001b[39m             starting_agent,\n\u001b[32m    415\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    416\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    417\u001b[39m             copy.deepcopy(prepared_input),\n\u001b[32m    418\u001b[39m             context_wrapper,\n\u001b[32m    419\u001b[39m         ),\n\u001b[32m    420\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    421\u001b[39m             agent=current_agent,\n\u001b[32m    422\u001b[39m             all_tools=all_tools,\n\u001b[32m    423\u001b[39m             original_input=original_input,\n\u001b[32m    424\u001b[39m             generated_items=generated_items,\n\u001b[32m    425\u001b[39m             hooks=hooks,\n\u001b[32m    426\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    427\u001b[39m             run_config=run_config,\n\u001b[32m    428\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    429\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    430\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    431\u001b[39m         ),\n\u001b[32m    432\u001b[39m     )\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    435\u001b[39m         agent=current_agent,\n\u001b[32m    436\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m         previous_response_id=previous_response_id,\n\u001b[32m    445\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:375\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    374\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    377\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:960\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m    958\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m    961\u001b[39m     agent,\n\u001b[32m    962\u001b[39m     system_prompt,\n\u001b[32m    963\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    964\u001b[39m     output_schema,\n\u001b[32m    965\u001b[39m     all_tools,\n\u001b[32m    966\u001b[39m     handoffs,\n\u001b[32m    967\u001b[39m     context_wrapper,\n\u001b[32m    968\u001b[39m     run_config,\n\u001b[32m    969\u001b[39m     tool_use_tracker,\n\u001b[32m    970\u001b[39m     previous_response_id,\n\u001b[32m    971\u001b[39m     prompt_config,\n\u001b[32m    972\u001b[39m )\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m    975\u001b[39m     agent=agent,\n\u001b[32m    976\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m    985\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m    986\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\run.py:1121\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, context_wrapper, run_config, tool_use_tracker, previous_response_id, prompt_config)\u001b[39m\n\u001b[32m   1118\u001b[39m model_settings = agent.model_settings.resolve(run_config.model_settings)\n\u001b[32m   1119\u001b[39m model_settings = RunImpl.maybe_reset_tool_choice(agent, tool_use_tracker, model_settings)\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1122\u001b[39m     system_instructions=system_prompt,\n\u001b[32m   1123\u001b[39m     \u001b[38;5;28minput\u001b[39m=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1124\u001b[39m     model_settings=model_settings,\n\u001b[32m   1125\u001b[39m     tools=all_tools,\n\u001b[32m   1126\u001b[39m     output_schema=output_schema,\n\u001b[32m   1127\u001b[39m     handoffs=handoffs,\n\u001b[32m   1128\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1129\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1130\u001b[39m     ),\n\u001b[32m   1131\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1132\u001b[39m     prompt=prompt_config,\n\u001b[32m   1133\u001b[39m )\n\u001b[32m   1135\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:65\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, prompt)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\n\u001b[32m     49\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     50\u001b[39m     system_instructions: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m     prompt: ResponsePromptParam | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     59\u001b[39m ) -> ModelResponse:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m generation_span(\n\u001b[32m     61\u001b[39m         model=\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model),\n\u001b[32m     62\u001b[39m         model_config=model_settings.to_json_dict() | {\u001b[33m\"\u001b[39m\u001b[33mbase_url\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m._client.base_url)},\n\u001b[32m     63\u001b[39m         disabled=tracing.is_disabled(),\n\u001b[32m     64\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m span_generation:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     66\u001b[39m             system_instructions,\n\u001b[32m     67\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     68\u001b[39m             model_settings,\n\u001b[32m     69\u001b[39m             tools,\n\u001b[32m     70\u001b[39m             output_schema,\n\u001b[32m     71\u001b[39m             handoffs,\n\u001b[32m     72\u001b[39m             span_generation,\n\u001b[32m     73\u001b[39m             tracing,\n\u001b[32m     74\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     75\u001b[39m             prompt=prompt,\n\u001b[32m     76\u001b[39m         )\n\u001b[32m     78\u001b[39m         message: ChatCompletionMessage | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     79\u001b[39m         first_choice: Choice | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\agents\\models\\openai_chatcompletions.py:273\u001b[39m, in \u001b[36mOpenAIChatCompletionsModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, span, tracing, stream, prompt)\u001b[39m\n\u001b[32m    267\u001b[39m store = ChatCmplHelpers.get_store_param(\u001b[38;5;28mself\u001b[39m._get_client(), model_settings)\n\u001b[32m    269\u001b[39m stream_options = ChatCmplHelpers.get_stream_options_param(\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_client(), model_settings, stream=stream\n\u001b[32m    271\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_client().chat.completions.create(\n\u001b[32m    274\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    275\u001b[39m     messages=converted_messages,\n\u001b[32m    276\u001b[39m     tools=converted_tools \u001b[38;5;129;01mor\u001b[39;00m NOT_GIVEN,\n\u001b[32m    277\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    278\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    279\u001b[39m     frequency_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.frequency_penalty),\n\u001b[32m    280\u001b[39m     presence_penalty=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.presence_penalty),\n\u001b[32m    281\u001b[39m     max_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    282\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    283\u001b[39m     response_format=response_format,\n\u001b[32m    284\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    285\u001b[39m     stream=stream,\n\u001b[32m    286\u001b[39m     stream_options=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(stream_options),\n\u001b[32m    287\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(store),\n\u001b[32m    288\u001b[39m     reasoning_effort=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(reasoning_effort),\n\u001b[32m    289\u001b[39m     extra_headers={**HEADERS, **(model_settings.extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})},\n\u001b[32m    290\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    291\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    292\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    293\u001b[39m     **(model_settings.extra_args \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    294\u001b[39m )\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, ChatCompletion):\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2454\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2411\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2412\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2413\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2451\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2452\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2453\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2455\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2456\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2457\u001b[39m             {\n\u001b[32m   2458\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2459\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2460\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2461\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2462\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2463\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2464\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2465\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2466\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2467\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2468\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2469\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2470\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2471\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2472\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2473\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2474\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2475\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2476\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2477\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2478\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2479\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2480\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2481\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2482\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2483\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2484\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2485\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2486\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2487\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2488\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2489\u001b[39m             },\n\u001b[32m   2490\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2491\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2492\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2493\u001b[39m         ),\n\u001b[32m   2494\u001b[39m         options=make_request_options(\n\u001b[32m   2495\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2496\u001b[39m         ),\n\u001b[32m   2497\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2498\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2499\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2500\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1791\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1779\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1786\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1787\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1788\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1789\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1790\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1591\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1588\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1590\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1591\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1593\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - [{'error': {'code': 400, 'message': '* GenerateContentRequest.generation_config.response_schema.properties[\"response\"].properties[\"jokes\"].properties: should be non-empty for OBJECT type\\n', 'status': 'INVALID_ARGUMENT'}}]"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "from agents import Agent, AgentOutputSchema, AgentOutputSchemaBase, Runner \n",
    "\n",
    "@dataclass\n",
    "class OutputType:\n",
    "    jokes: dict[int, str]\n",
    "    \"\"\"A list of jokes, indexed by joke number.\"\"\"\n",
    "\n",
    "\n",
    "class CustomOutputSchema(AgentOutputSchemaBase):\n",
    "    \"\"\"A demonstration of a custom output schema.\"\"\"\n",
    "\n",
    "    def is_plain_text(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def name(self) -> str:\n",
    "        return \"CustomOutputSchema\"\n",
    "\n",
    "    def json_schema(self) -> dict[str, Any]:\n",
    "        return {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\"jokes\": {\"type\": \"object\", \"properties\": {\"joke\": {\"type\": \"string\"}}}},\n",
    "        }\n",
    "\n",
    "    def is_strict_json_schema(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    def validate_json(self, json_str: str) -> Any:\n",
    "        json_obj = json.loads(json_str)\n",
    "        # Just for demonstration, we'll return a list.\n",
    "        return list(json_obj[\"jokes\"].values())\n",
    "\n",
    "\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Assistant\",\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        output_type=OutputType,model=model\n",
    "    )\n",
    "\n",
    "    input = \"Tell me 3 short jokes.\"\n",
    "\n",
    "    # First, let's try with a strict output type. This should raise an exception.\n",
    "    try:\n",
    "        result = await Runner.run(agent, input)\n",
    "        raise AssertionError(\"Should have raised an exception\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error (expected): {e}\")\n",
    "\n",
    "    # Now let's try again with a non-strict output type. This should work.\n",
    "    # In some cases, it will raise an error - the schema isn't strict, so the model may\n",
    "    # produce an invalid JSON object.\n",
    "    agent.output_type = AgentOutputSchema(OutputType, strict_json_schema=False)\n",
    "    result = await Runner.run(agent, input)\n",
    "    print(result.final_output)\n",
    "\n",
    "    # Finally, let's try a custom output type.\n",
    "    agent.output_type = CustomOutputSchema()\n",
    "    result = await Runner.run(agent, input)\n",
    "    print(result.final_output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8071d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] get_weather called\n",
      "OK. The weather in Tokyo is sunny with wind. The temperature is between 14 and 20 degrees Celsius.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from agents import Agent, Runner, function_tool\n",
    "\n",
    "\n",
    "class Weather(BaseModel):\n",
    "    city: str\n",
    "    temperature_range: str\n",
    "    conditions: str\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> Weather:\n",
    "    print(\"[debug] get_weather called\")\n",
    "    return Weather(city=city, temperature_range=\"14-20C\", conditions=\"Sunny with wind.\")\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Hello world\",\n",
    "    instructions=\"You are a helpful agent.\",\n",
    "    tools=[get_weather],model=model\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(agent, input=\"What's the weather in Tokyo?\")\n",
    "    print(result.final_output)\n",
    "    # The weather in Tokyo is sunny.\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faeb647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] get_weather called\n",
      "city='Tokyo' temperature_range='14-20C' conditions='Sunny with wind.'\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from agents import Agent, Runner, function_tool\n",
    "\n",
    "\n",
    "class Weather(BaseModel):\n",
    "    city: str\n",
    "    temperature_range: str\n",
    "    conditions: str\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> Weather:\n",
    "    print(\"[debug] get_weather called\")\n",
    "    return Weather(city=city, temperature_range=\"14-20C\", conditions=\"Sunny with wind.\")\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Hello world\",\n",
    "    instructions=\"You are a helpful agent.\",\n",
    "    tools=[get_weather],model=model,tool_use_behavior='stop_on_first_tool'\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(agent, input=\"What's the weather in Tokyo?\")\n",
    "    print(result.final_output)\n",
    "    # The weather in Tokyo is sunny.\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54c12490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] get_weather called\n",
      "<class '__main__.Weather'> city='Tokyo' temperature_range='14-20C' conditions='Sunny with wind.'\n",
      "The weather in Tokyo is sunny with wind, and the temperature is between 14-20C.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from agents import Agent, Runner, function_tool\n",
    "\n",
    "\n",
    "class Weather(BaseModel):\n",
    "    city: str\n",
    "    temperature_range: str\n",
    "    conditions: str\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> Weather:\n",
    "    print(\"[debug] get_weather called\")\n",
    "    res= Weather(city=city, temperature_range=\"14-20C\", conditions=\"Sunny with wind.\")\n",
    "    print(type(res),res)\n",
    "    return res\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Hello world\",\n",
    "    instructions=\"You are a helpful agent.\",\n",
    "    tools=[get_weather],model=model,tool_use_behavior='run_llm_again'\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    result = await Runner.run(agent, input=\"What's the weather in Tokyo?\")\n",
    "    print(result.final_output)\n",
    "    # The weather in Tokyo is sunny.\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82770a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 done\n",
      "Step 2 done\n",
      "Step 3 done\n",
      "result after second_agent:I do not have access to real-time information about population data.\n",
      "\n",
      "===Final messages===\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stream_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01masyncio\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\code\\open_ai_agent_sdk\\.venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m===Final messages===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# 5. That should have caused spanish_handoff_message_filter to be called, which means the\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# output should be missing the first two messages, and have no tool calls.\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Let's print the messages to see what happened\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstream_result\u001b[49m.to_input_list():\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mprint\u001b[39m(json.dumps(item, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'stream_result' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "from agents import Agent, HandoffInputData, Runner, function_tool, handoff, trace\n",
    "from agents.extensions import handoff_filters\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def random_number_tool(max: int) -> int:\n",
    "    \"\"\"Return a random integer between 0 and the given maximum.\"\"\"\n",
    "    return random.randint(0, max)\n",
    "\n",
    "\n",
    "def spanish_handoff_message_filter(handoff_message_data: HandoffInputData) -> HandoffInputData:\n",
    "    # First, we'll remove any tool-related messages from the message history\n",
    "    handoff_message_data = handoff_filters.remove_all_tools(handoff_message_data)\n",
    "    print(f'handoff_message_data:',handoff_message_data)\n",
    "    # Second, we'll also remove the first two items from the history, just for demonstration\n",
    "    history = (\n",
    "        tuple(handoff_message_data.input_history[2:])\n",
    "        if isinstance(handoff_message_data.input_history, tuple)\n",
    "        else handoff_message_data.input_history\n",
    "    )\n",
    "    print(f'history:{history}')\n",
    "\n",
    "    return HandoffInputData(\n",
    "        input_history=history,\n",
    "        pre_handoff_items=tuple(handoff_message_data.pre_handoff_items),\n",
    "        new_items=tuple(handoff_message_data.new_items),\n",
    "    )\n",
    "\n",
    "\n",
    "first_agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"Be extremely concise.\",\n",
    "    tools=[random_number_tool],model=model\n",
    ")\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"Spanish Assistant\",\n",
    "    instructions=\"You only speak Spanish and are extremely concise.\",\n",
    "    handoff_description=\"A Spanish-speaking assistant.\",model=model\n",
    ")\n",
    "\n",
    "second_agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=(\n",
    "        \"Be a helpful assistant. If the user speaks Spanish, handoff to the Spanish assistant.\"\n",
    "    ),\n",
    "    handoffs=[handoff(spanish_agent, input_filter=spanish_handoff_message_filter)],model=model\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Trace the entire run as a single workflow\n",
    "    with trace(workflow_name=\"Streaming message filter\"): \n",
    "        result = await Runner.run(first_agent, input=\"Hi, my name is Sora.\")\n",
    "        print(\"Step 1 done\") \n",
    "        result = await Runner.run(\n",
    "            first_agent,\n",
    "            input=result.to_input_list()\n",
    "            + [{\"content\": \"Can you generate a random number between 0 and 100?\", \"role\": \"user\"}],\n",
    "        )\n",
    "\n",
    "        print(\"Step 2 done\")\n",
    "\n",
    "        # 3. Call the second agent\n",
    "        result = await Runner.run(\n",
    "            second_agent,\n",
    "            input=result.to_input_list()\n",
    "            + [\n",
    "                {\n",
    "                    \"content\": \"I live in New York City. Whats the population of the city?\",\n",
    "                    \"role\": \"user\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        print(\"Step 3 done\")\n",
    "        print(f'result after second_agent:{result.final_output}')\n",
    "        \n",
    "        # # 4. Cause a handoff to occur\n",
    "        # stream_result = Runner.run_streamed(\n",
    "        #     second_agent,\n",
    "        #     input=result.to_input_list()\n",
    "        #     + [\n",
    "        #         {\n",
    "        #             \"content\": \"Por favor habla en español. ¿Cuál es mi nombre y dónde vivo?\",\n",
    "        #             \"role\": \"user\",\n",
    "        #         }\n",
    "        #     ],\n",
    "        # )\n",
    "        # async for _ in stream_result.stream_events():\n",
    "        #     pass\n",
    "\n",
    "        # print(\"Step 4 done\")\n",
    "\n",
    "    print(\"\\n===Final messages===\\n\")\n",
    "\n",
    "    # 5. That should have caused spanish_handoff_message_filter to be called, which means the\n",
    "    # output should be missing the first two messages, and have no tool calls.\n",
    "    # Let's print the messages to see what happened\n",
    "    for item in stream_result.to_input_list():\n",
    "        print(json.dumps(item, indent=2)) \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e425d848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 done\n",
      "Step 2 done\n",
      "Step 3 done\n",
      "result after second_agent:Sorry, I do not have the ability to look up the population of New York City.\n",
      "\n",
      "handoff_message_data: HandoffInputData(input_history=({'content': 'Hi, my name is Sora.', 'role': 'user'}, {'id': '__fake_id__', 'content': [{'annotations': [], 'text': 'Nice to meet you, Sora.\\n', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}, {'content': 'Can you generate a random number between 0 and 100?', 'role': 'user'}, {'id': '__fake_id__', 'content': [{'annotations': [], 'text': '71\\n', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}, {'content': 'I live in New York City. Whats the population of the city?', 'role': 'user'}, {'id': '__fake_id__', 'content': [{'annotations': [], 'text': 'Sorry, I do not have the ability to look up the population of New York City.\\n', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}, {'content': 'Por favor habla en español. ¿Cuál es mi nombre y dónde vivo?', 'role': 'user'}), pre_handoff_items=(), new_items=())\n",
      "history:({'content': 'Can you generate a random number between 0 and 100?', 'role': 'user'}, {'id': '__fake_id__', 'content': [{'annotations': [], 'text': '71\\n', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}, {'content': 'I live in New York City. Whats the population of the city?', 'role': 'user'}, {'id': '__fake_id__', 'content': [{'annotations': [], 'text': 'Sorry, I do not have the ability to look up the population of New York City.\\n', 'type': 'output_text'}], 'role': 'assistant', 'status': 'completed', 'type': 'message'}, {'content': 'Por favor habla en español. ¿Cuál es mi nombre y dónde vivo?', 'role': 'user'})\n",
      "Step 4 done\n",
      "\n",
      "===Final messages===\n",
      "\n",
      "{\n",
      "  \"content\": \"Can you generate a random number between 0 and 100?\",\n",
      "  \"role\": \"user\"\n",
      "}\n",
      "{\n",
      "  \"id\": \"__fake_id__\",\n",
      "  \"content\": [\n",
      "    {\n",
      "      \"annotations\": [],\n",
      "      \"text\": \"71\\n\",\n",
      "      \"type\": \"output_text\"\n",
      "    }\n",
      "  ],\n",
      "  \"role\": \"assistant\",\n",
      "  \"status\": \"completed\",\n",
      "  \"type\": \"message\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"I live in New York City. Whats the population of the city?\",\n",
      "  \"role\": \"user\"\n",
      "}\n",
      "{\n",
      "  \"id\": \"__fake_id__\",\n",
      "  \"content\": [\n",
      "    {\n",
      "      \"annotations\": [],\n",
      "      \"text\": \"Sorry, I do not have the ability to look up the population of New York City.\\n\",\n",
      "      \"type\": \"output_text\"\n",
      "    }\n",
      "  ],\n",
      "  \"role\": \"assistant\",\n",
      "  \"status\": \"completed\",\n",
      "  \"type\": \"message\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"Por favor habla en espa\\u00f1ol. \\u00bfCu\\u00e1l es mi nombre y d\\u00f3nde vivo?\",\n",
      "  \"role\": \"user\"\n",
      "}\n",
      "{\n",
      "  \"id\": \"__fake_id__\",\n",
      "  \"content\": [\n",
      "    {\n",
      "      \"annotations\": [],\n",
      "      \"text\": \"No tengo esa informaci\\u00f3n. No s\\u00e9 qui\\u00e9n eres ni d\\u00f3nde vives.\\n\",\n",
      "      \"type\": \"output_text\"\n",
      "    }\n",
      "  ],\n",
      "  \"role\": \"assistant\",\n",
      "  \"status\": \"completed\",\n",
      "  \"type\": \"message\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "from agents import Agent, HandoffInputData, Runner, function_tool, handoff, trace\n",
    "from agents.extensions import handoff_filters\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def random_number_tool(max: int) -> int:\n",
    "    \"\"\"Return a random integer between 0 and the given maximum.\"\"\"\n",
    "    return random.randint(0, max)\n",
    "\n",
    "\n",
    "def spanish_handoff_message_filter(handoff_message_data: HandoffInputData) -> HandoffInputData:\n",
    "    # First, we'll remove any tool-related messages from the message history\n",
    "    handoff_message_data = handoff_filters.remove_all_tools(handoff_message_data)\n",
    "    print(f'handoff_message_data:',handoff_message_data)\n",
    "    # Second, we'll also remove the first two items from the history, just for demonstration\n",
    "    history = (\n",
    "        tuple(handoff_message_data.input_history[2:])\n",
    "        if isinstance(handoff_message_data.input_history, tuple)\n",
    "        else handoff_message_data.input_history\n",
    "    )\n",
    "    print(f'history:{history}')\n",
    "\n",
    "    return HandoffInputData(\n",
    "        input_history=history,\n",
    "        pre_handoff_items=tuple(handoff_message_data.pre_handoff_items),\n",
    "        new_items=tuple(handoff_message_data.new_items),\n",
    "    )\n",
    "\n",
    "\n",
    "first_agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"Be extremely concise.\",\n",
    "    tools=[random_number_tool],model=model\n",
    ")\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"Spanish Assistant\",\n",
    "    instructions=\"You only speak Spanish and are extremely concise.\",\n",
    "    handoff_description=\"A Spanish-speaking assistant.\",model=model\n",
    ")\n",
    "\n",
    "second_agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=(\n",
    "        \"Be a helpful assistant. If the user speaks Spanish, handoff to the Spanish assistant.\"\n",
    "    ),\n",
    "    handoffs=[handoff(spanish_agent, input_filter=spanish_handoff_message_filter)],model=model\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    # Trace the entire run as a single workflow\n",
    "    with trace(workflow_name=\"Streaming message filter\"): \n",
    "        result = await Runner.run(first_agent, input=\"Hi, my name is Sora.\")\n",
    "        print(\"Step 1 done\") \n",
    "        result = await Runner.run(\n",
    "            first_agent,\n",
    "            input=result.to_input_list()\n",
    "            + [{\"content\": \"Can you generate a random number between 0 and 100?\", \"role\": \"user\"}],\n",
    "        )\n",
    "\n",
    "        print(\"Step 2 done\")\n",
    "\n",
    "        # 3. Call the second agent\n",
    "        result = await Runner.run(\n",
    "            second_agent,\n",
    "            input=result.to_input_list()\n",
    "            + [\n",
    "                {\n",
    "                    \"content\": \"I live in New York City. Whats the population of the city?\",\n",
    "                    \"role\": \"user\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        print(\"Step 3 done\")\n",
    "        print(f'result after second_agent:{result.final_output}')\n",
    "        \n",
    "        # 4. Cause a handoff to occur\n",
    "        stream_result = Runner.run_streamed(\n",
    "            second_agent,\n",
    "            input=result.to_input_list()\n",
    "            + [\n",
    "                {\n",
    "                    \"content\": \"Por favor habla en español. ¿Cuál es mi nombre y dónde vivo?\",\n",
    "                    \"role\": \"user\",\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "        async for _ in stream_result.stream_events():\n",
    "            pass\n",
    "\n",
    "        print(\"Step 4 done\")\n",
    "\n",
    "    print(\"\\n===Final messages===\\n\")\n",
    "\n",
    "    # 5. That should have caused spanish_handoff_message_filter to be called, which means the\n",
    "    # output should be missing the first two messages, and have no tool calls.\n",
    "    # Let's print the messages to see what happened\n",
    "    for item in stream_result.to_input_list():\n",
    "        print(json.dumps(item, indent=2)) \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c134cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
